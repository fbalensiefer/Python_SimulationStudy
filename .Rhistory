Y = beta[1] + beta[2]*X + beta[3]*(X^2) + beta[4]*(X^3) + eps
rm(eps)
df=as.data.frame(cbind(Y,X))
#####################################################################
###   Exercise 1
## a) computing SE and CI by relying on 2*SE
model = df$Y ~ df$X + df$X^2 + df$X^3
ols = lm(model)
rslt  = summary(ols)
se    = rslt$residuals
estfit= predict(ols, df)
upper  = estfit+2*se
lower = estfit-2*se
# sorting values
Y=sort(Y)
X=sort(X)
L=sort(lower)
U=sort(upper)
#par(mfrow=c(1,2))
plot(X,Y)
lines(X, L, col="red",lty=2)
lines(X, U, col="red",lty=2)
## b) computing Bootstraped CI's
bootfunc <- function(data, index) {
coefficients(lm(Y ~ X + X^2 + X^3 , data = data, subset = index))
}
set.seed(1)
bootstrap = boot(df, bootfunc, 1000)
View(bootstrap)
#####################################################################
###   Group 10 - ProblemSet 06
#####################################################################
### Preface
# clearing workspace and set wd
rm(list=ls())
cat("\014")
dev.off()
#library(plm)        # Panel Methods - regression models
#library(mvtnorm)     # random draws from a multivariate normal distr.
#library(MASS)       # to fit LDA and QDA analysis
#library(glmnet)     # Lasso and Ridge regression
library(boot)        # Bootstrap
# Note: packages above require the data to be saved as a data frame
# data config
N  = 500
mu = 0
covmat = 1
## DGP
# Variance-Covariance Matrix, beta vector
set.seed(123)
beta=c(1,1.5,-1.5,1.5)
# sampling data and generate y's
X = rnorm(N, mu, covmat)
#con = rep(1,N)
#X   = cbind(con,X)
eps = rnorm(N, 0, 1^0.5)
Y = beta[1] + beta[2]*X + beta[3]*(X^2) + beta[4]*(X^3) + eps
rm(eps)
df=as.data.frame(cbind(Y,X))
#####################################################################
###   Exercise 1
## a) computing SE and CI by relying on 2*SE
model = df$Y ~ df$X + (df$X^2) + (df$X^3)
ols = lm(model)
rslt  = summary(ols)
se    = rslt$residuals
estfit= predict(ols, df)
upper  = estfit+2*se
lower = estfit-2*se
# sorting values
Y=sort(Y)
X=sort(X)
L=sort(lower)
U=sort(upper)
#par(mfrow=c(1,2))
plot(X,Y)
lines(X, L, col="red",lty=2)
lines(X, U, col="red",lty=2)
## b) computing Bootstraped CI's
bootfunc <- function(data, index) {
coefficients(lm( Y ~ X + (X^2) + (X^3) , data = data, subset = index))
}
set.seed(1)
bootstrap = boot(df, bootfunc, 1000)
boot.out()
boot.ci(bootstrap)
plot(bootstrap)
bootfunc <- function(data, index) {
coefficients(lm(Y ~ X+(X^2)+(X^3) , data = data, subset = index))
}
set.seed(1)
bootstrap = boot(df, bootfunc, 1000)
reults=bootstrap
plot(results, index=1) # intercept
plot(results, index=2) # wt
plot(results, index=3) # disp
results=bootstrap
plot(results, index=1) # intercept
plot(results, index=2) # wt
plot(results, index=3) # disp
qunatile(X, prob=0.05)
quantile(X, prob=0.05)
quantile(X, prob=0.99)
#####################################################################
###   ProblemSet 01: DGP, MonteCarlo and MTE
#####################################################################
### Preface
# clearing workspace and set wd
rm(list=ls())
cat("\014")
dev.off()
#library(plm)
#####################################################################
###   Exercise 1
N=1000
sig_x= 1.5
mu_x=  0
sig_u= 10
beta1= 5
beta2= -0.5
x_1= c(rep(1,N))      # is a constant
set.seed(123)         # to generate always the same random numbers
#a)
x_2_train=rnorm(N,mu_x,sig_x^0.5)
res_train=rnorm(N,0,sig_u^0.5)
y_train=beta1*x_1+beta2*x_2_train+res_train
#b)
x_2_test=rnorm(N,mu_x,sig_x^0.5)
res_test=rnorm(N,0,sig_u^0.5)
y_test=beta1*x_1+beta2*x_2_test+res_test
#c)
model=lm(y_train ~ x_2_train)
summary(model)
beta1_hat=unname(model$coefficients[1])
beta2_hat=unname(model$coefficients[2])
#d)
yfit_train=predict(model)
MSE=1/N*sum((y_train-yfit_train)^2)
yfit_test=beta1_hat+beta2_hat*x_2_test
AVE=1/N*sum((y_test-yfit_test)^2)     # Ave=MSE but only for compairing training and test sample
# e)
#defining additional X_2T values with increasing polynomial
x_2_train.poly=matrix(NaN,N,5) #empty matrix to be filled with polynomials of x_2T
for (i in 0:4){
x_2_train.poly[,i+1]=x_2_train^i #column 1-5 filled with x_2T to the power of 0-4
}
#defining coefficients for each regression on additional polynomial of x_2T
OLS.poly=matrix(0,5,5) #empty matrix to be filled with coefficients of regressions
yfit_train.poly=matrix(NaN,N,5) #empty matrix to be filled with fitted values of regressions
for (i in 0:4){
OLS.poly[1:(i+1),(i+1)]=lm(y_train~x_2_train.poly[,1:(i+1)]-1)$coefficients
yfit_train.poly[,(i+1)]=lm(y_train~x_2_train.poly[,1:(i+1)]-1)$fitted.values
}
#calculating MSEs with increasing polynomial
MSE.poly=numeric()
for (i in 1:5){
MSE.poly[i]=(1/N)*sum((y_train-yfit_train.poly[,i])^2)
}
MSE.poly
#calculating predited values for test sample
yfit_test.poly=matrix(NaN,N,5)
for (i in 1:5){
yfit_test.poly[,i]=OLS.poly[1,i]*x_1+OLS.poly[2,i]*x_2_test+OLS.poly[3,i]*(x_2_test^2)+
OLS.poly[4,i]*(x_2_test^3)+OLS.poly[5,i]*(x_2_test^4)
}
#calculating AVEs with increasing polynomial
AVE.poly=numeric()
for (i in 1:5){
AVE.poly[i]=(1/N)*sum((y_test-yfit_test.poly[,i])^2)
}
AVE.poly
par(mfrow=c(1,2))
plot(MSE.poly, type='l', col="red")
plot(AVE.poly, type='l', col="red")
#####################################################################
###   Exercise 2
rm(list=ls())
#cat("\014")
N=1000
sig_x= 1.5
mu_x=  0
sig_u= 10
beta1= 5
beta2= -0.5
x_1= c(rep(1,N))      # is a constant
# a) and b)
MCN=1000
MSE_Ex2=matrix(NaN,MCN,1)
AVE_Ex2=matrix(NaN,MCN,1)
set.seed(100)
### Note: For efficieny it is better to generatetest sample once
#         before the MC-Simulation is started.
#         Reason: training and test sample are drawn from a random distribution
# test Data
x_2_test=rnorm(N,mu_x,sig_x^0.5)
res_test=rnorm(N,0,sig_u^0.5)
y_test=beta1*x_1+beta2*x_2_test+res_test
for (i in 1:MCN){
# training Data
x_2_train=rnorm(N,mu_x,sig_x^0.5)
res_train=rnorm(N,0,sig_u^0.5)
y_train=beta1*x_1+beta2*x_2_train+res_train
# setting up regression model
model=lm(y_train ~ x_2_train)
beta1_hat=unname(model$coefficients[1])
beta2_hat=unname(model$coefficients[2])
# calculation of MSE
yfit_train=predict(model)
MSE_Ex2[i]=1/N*sum((y_train-yfit_train)^2)
# calculation of AVE
yfit_test=beta1_hat+beta2_hat*x_2_test
AVE_Ex2[i]=1/N*sum((y_test-yfit_test)^2)
}
# b)
avg_MSE=mean(MSE_Ex2)
avg_AVE=mean(AVE_Ex2)
# c)
par(mfrow=c(1,2))
plot(MSE_Ex2, type="l", col="blue")
abline(h=avg_MSE, col="black")
plot(AVE_Ex2, type="l", col="red")
abline(h=avg_AVE, col="black")
# d) Along which margins could you vary parameters of the initial simulation set-up and what would
#    be your intuition based on the theoretical properties of the considered objects of interest?
#
#    Idea:  the higher the number of iterations in our Monte-Carlo simulation, the more precise will be our estimation
#           of avg. MSE and avg. AVE to the true DGP value sigma_u
#    For example: MCN=10 vs. MCN=1000
#####################################################################
###   ProblemSet 02: LDA and QDA
#####################################################################
### Preface
# clearing workspace and set wd
rm(list=ls())
cat("\014")
dev.off()
#library(plm)       # Panel Methods - regression models
#library(mvtnorm)   # random draws from a multivariate normal distr.
library(MASS)       # to fit LDA and QDA analysis
# Note: packages above require the data to be saved as a data frame
#####################################################################
###   Exercise 1
# class 1
n1 = 300
mu1= c(-3,3)
# class 2
n2 = 500
mu2= c(5,5)
# Variance-Covariance Matrix
covmat=matrix(c(16,-2,-2,9), nrow=2, ncol=2)
#total number of observations
N=n1+n2
## a)
# DGP - for both classes
set.seed(123)
X1=mvrnorm(n = n1, mu = mu1, Sigma = covmat)
X2=mvrnorm(n = n2, mu = mu2, Sigma = covmat)
df1 <- data.frame(X1)
df1['class'] = 1
df2 <- data.frame(X2)
df2['class'] = 2
df  <- merge(df1,df2, all=TRUE)
rm(df1, df2, X1, X2)
## b)
mod_lda  =  lda(class ~ X1 + X2, data=df)
summary(mod_lda)
class_lfit  <- as.numeric(predict(mod_lda)$class)
mod_qda  =  qda(class ~ X1 + X2 , data=df)
summary(mod_qda)
class_qfit  <- as.numeric(predict(mod_qda)$class)
## c)
# Note: since classes are ordinal scale we can not use MSE, due to the fact
#       that the distance between class 1 and 3 are the same as between 1 and 2
#       furthermore it is not appropriarte to use OLS
MTE_LDA=sum(class_lfit!=df$class)/N
MTE_QDA=sum(class_qfit!=df$class)/N
print(MTE_LDA)
print(MTE_QDA)
print(MTE_LDA-MTE_QDA)
# a)
rm(list=ls())
cat("\014")
MCN=100
MSE=matrix(NaN,MCN,2)
n1 = 300
mu1= c(-3,3)
n2 = 500
mu2= c(5,5)
covmat=matrix(c(16,-2,-2,9), nrow=2, ncol=2)
N=n1+n2
set.seed(123)
for (i in 1:MCN){
X1=mvrnorm(n = n1, mu = mu1, Sigma = covmat)
X2=mvrnorm(n = n2, mu = mu2, Sigma = covmat)
df1 <- data.frame(X1)
df1['class'] = 1
df2 <- data.frame(X2)
df2['class'] = 2
df  <- merge(df1,df2, all=TRUE)
mod_lda  =  lda(class ~ X1 + X2, data=df)
class_lfit  <- as.numeric(predict(mod_lda)$class)
mod_qda  =  qda(class ~ X1 + X2 , data=df)
class_qfit  <- as.numeric(predict(mod_qda)$class)
MSE[i,1]=sum(class_lfit!=df$class)/N
MSE[i,2]=sum(class_qfit!=df$class)/N
}
avg_MSE_LDA=mean(MSE[,1])
avg_MSE_QDA=mean(MSE[,2])
#par(mfrow=c(1,2))
#plot(MSE[,1], ylab="LDA")
#abline(h=avg_MSE_LDA, col="red")
#plot(MSE[,2], ylab="QDA")
#abline(h=avg_MSE_QDA, col="red")
summary(avg_MSE_LDA-avg_MSE_QDA)
par(mfrow=c(1,2))
plot(MSE[,1], ylab="LDA")
abline(h=avg_MSE_LDA, col="red")
plot(MSE[,2], ylab="QDA")
abline(h=avg_MSE_QDA, col="red")
# b)
# Note: since we have different covariate matrixes sigma 1 and sigma 2
#       QDA is more precise than LDA
#       From a theoretical perspective:
#       * if LDAs assumption that the K classes share a common covariance matrix
#          is badly off, then LDA --> high bias
#       * LDA is a much less flexible classifier than QDA --> lower variance
# Hence: Try with different covariance matrices
rm(list=ls())
cat("\014")
MCN=100
MSE=matrix(NaN,MCN,2)
n1 = 300
mu1= c(-3,3)
n2 = 500
mu2= c(5,5)
covmat_1=matrix(c(16,-2,-2,9), nrow=2, ncol=2)
covmat_2=matrix(c(10,-2,-2,5), nrow=2, ncol=2)
N=n1+n2
set.seed(123)
for (i in 1:MCN){
X1=mvrnorm(n = n1, mu = mu1, Sigma = covmat_1)
X2=mvrnorm(n = n2, mu = mu2, Sigma = covmat_2)
df1 <- data.frame(X1)
df1['class'] = 1
df2 <- data.frame(X2)
df2['class'] = 2
df  <- merge(df1,df2, all=TRUE)
mod_lda  =  lda(class ~ X1 + X2, data=df)
class_lfit  <- as.numeric(predict(mod_lda)$class)
mod_qda  =  qda(class ~ X1 + X2 , data=df)
class_qfit  <- as.numeric(predict(mod_qda)$class)
MSE[i,1]=sum(class_lfit!=df$class)/N
MSE[i,2]=sum(class_qfit!=df$class)/N
}
avg_MSE_LDA=mean(MSE[,1])
avg_MSE_QDA=mean(MSE[,2])
par(mfrow=c(1,2))
plot(MSE[,1], ylab="LDA")
abline(h=avg_MSE_LDA, col="red")
plot(MSE[,2], ylab="QDA")
abline(h=avg_MSE_QDA, col="red")
summary(avg_MSE_LDA-avg_MSE_QDA)
###Simulating OLS samples and plot the regression lines####
#set.seed(32323)
## Two explanatory variables plus an intercept:
N         <- 200 # Number of observations
X.1       <- rep(1, N)
X.2      <- rnorm(N, mean=0, sd=1) # (pseudo) random numbers form a normal distr
X         <- cbind(X.1, X.2,X.2^2,X.2^3)
###Homoscedastic error term
eps       <-rnorm(N,0,10)#
beta.vec  <- c(1,1.5,-1.5,1.5)
y         <- X %*% beta.vec + eps
##Solving for beta hat###
beta.hat <- solve(t(X) %*% X) %*% t(X) %*% y
beta.hat
x.grid<-sort(X.2) #
true_model<-beta.vec[1]+beta.vec[2]*x.grid+beta.vec[3]*x.grid^2+beta.vec[4]*x.grid^3
estim_model<-beta.hat[1]+beta.hat[2]*x.grid+beta.hat[3]*x.grid^2+beta.hat[4]*x.grid^3
plot(estim_model,type="l")
lines(true_model,type="l",lty=1,col="red")
####Calculate the covariance matrix###
#####calculate the fitted values#####
y.hat<- X %*% beta.hat
eps.hat<-y-X %*% beta.hat
###calculate the covariance matrix#
se<-(t(eps.hat)%*%(eps.hat))/(N-5)
cov<-se[1]*solve(t(X) %*% X)
new_data<-X[order(X[,2]),]
#new_data<-x.grid
var<-c()
for(i in 1:N)
{
var[i]=t(new_data[i,])%*%cov%*%(new_data[i,])
}
d<-2*(sqrt(var))
estim<-beta.hat[1]+beta.hat[2]*new_data[,2]+beta.hat[3]*new_data[,2]^2+beta.hat[4]*new_data[,2]^3
conf_low<-estim-d
conf_high<-estim+d
plot(estim,type="l")
lines(conf_low,lty=2,col="red")
lines(conf_high,lty=2,col="blue")
######Bootstrapped confidence intervals#####################
B=200
b_sample<-matrix(NA,N,B)
beta.hat<-matrix(NA,length(X),B)
data<-cbind(y,X)
function_B<-matrix(NA,length(x.grid),B)
for(i in 1:B)
{
XB<- data[sample(nrow(data),replace=T, N), ]
beta.hat<-solve(t(XB[,2:5]) %*% XB[,2:5]) %*% t(XB[,2:5]) %*%XB[,1]
function_B[,i]<-beta.hat[1]+beta.hat[2]*x.grid+beta.hat[3]*x.grid^2+beta.hat[4]*x.grid^3
}
plot(function_B[,1])
plot(function_B[,2])
View(function_B)
sorted<-t(apply(function_B,1,sort))
####Calculate the empirical quantiles####################
conf_highB<-sorted[,195]
conf_lowB<-sorted[,5]
###########################################
plot(estim,type="l",lty=3)
lines(conf_highB)
lines(conf_lowB)
lines(conf_low,lty=2,col="red")
lines(conf_high,lty=2,col="blue")
lines(true_model,col="green")
####Calculate the coverage probabilities#################################
cov_prob_B<-true_model>conf_lowB&true_model<conf_highB
sum(cov_prob_B)/length(x.grid)
cov_prob_A<-true_model>conf_low&true_model<conf_high
sum(cov_prob_A)/length(x.grid)
panel_sample <- read.csv("C:/Users/fabia/Google Drive/UniBonn/X_Microeconometrics/student-project-fbalensiefer/panel_sample.csv")
View(panel_sample)
model='y ~ '
panel_sample['dd']=M*Exp
panel_sample['dd']=panel_sample$M*panel_sample$Exp
model='Y ~ dd + c(iID) + c(group_timeID)'
reg=lm(model,panel_sample)
reg.summary()
summary(reg)
model='Y ~ dd + c(iID) + c(group_timeID)-1'
reg=lm(model,panel_sample)
summary(reg)
View(panel_sample)
View(panel_sample)
model='Y ~ dd + X + L + E + c(iID) + c(group_timeID)'
reg=lm(model,panel_sample)
summary(reg)
model='Y ~ dd + c(indivID) + c(group_timeID)'
reg=lm(model,panel_sample)
summary(reg)
model='Y ~ dd + C(indivID) + C(group_timeID)'
reg=lm(model,panel_sample)
model='Y ~ dd + c(indivID) + c(group_timeID)'
reg=lm(model,panel_sample)
summary(reg)
panel_sample <- read.csv("C:/Users/fabia/Google Drive/UniBonn/X_Microeconometrics/student-project-fbalensiefer/panel_sample.csv")
panel_sample['DD']=df$M*df$Exp
model='Y ~ dd + c(indivID) + c(group_timeID)'
reg=lm(model,panel_sample)
reg.summary()
rm(list=ls())
#setwd('/Users/Fabian/Google Drive/UniBonn/X_Microeconometrics/student-project-fbalensiefer')
setwd('C:/Users/fabia/Google Drive/UniBonn/X_Microeconometrics/student-project-fbalensiefer')
# simulation sample
panel_sample <- read.csv("C:/Users/fabia/Google Drive/UniBonn/X_Microeconometrics/student-project-fbalensiefer/panel_sample.csv")
panel_sample['DD']=df$M*df$Exp
model='Y ~ dd + c(indivID) + c(group_timeID)'
reg=lm(model,panel_sample)
reg.summary()
###############################################################################
rm(list=ls())
#setwd('/Users/Fabian/Google Drive/UniBonn/X_Microeconometrics/student-project-fbalensiefer')
setwd('C:/Users/fabia/Google Drive/UniBonn/X_Microeconometrics/student-project-fbalensiefer')
# simulation sample
panel_sample <- read.csv("C:/Users/fabia/Google Drive/UniBonn/X_Microeconometrics/student-project-fbalensiefer/panel_sample.csv")
panel_sample['DD']=panel_sample$M*panel_sample$Exp
model='Y ~ DD + c(indivID) + c(group_timeID)'
reg=lm(model,panel_sample)
reg.summary()
summary(reg)
# preface loading packages required for R
library(plm)
library(lfe)
summary(felm('Y~DD| indivID + group_timeID', data=panel_sample))
summary(felm('Y ~ DD | indivID + group_timeID', data=panel_sample))
femod=felm('Y ~ DD | indivID + group_timeID', data=panel_sample)
summary(femod)
femod=felm(Y ~ DD | indivID + group_timeID, data=panel_sample)
summary(femod)
###############################################################################
rm(list=ls())
#setwd('/Users/Fabian/Google Drive/UniBonn/X_Microeconometrics/student-project-fbalensiefer')
setwd('C:/Users/fabia/Google Drive/UniBonn/X_Microeconometrics/student-project-fbalensiefer')
# preface loading packages required for R
library(plm)
library(lfe)
#library(haven)
library(dplyr)
library(dummies)
library(data.table)
#library(fastDummies)
# simulation sample
panel_sample <- read.csv("C:/Users/fabia/Google Drive/UniBonn/X_Microeconometrics/student-project-fbalensiefer/panel_sample.csv")
panel_sample['DD']=panel_sample$M*panel_sample$Exp
model='Y ~ DD + c(indivID) + c(group_timeID)'
reg=lm(model,panel_sample)
summary(reg)
femod=felm(Y ~ DD | indivID + group_timeID, data=panel_sample)
summary(femod)
