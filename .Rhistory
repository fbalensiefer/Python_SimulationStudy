Y = X %*% beta + eps
#### a)
# Step 1) Calculate cov(X)
A=cov(X)
# Step 2) Calculate Eigenvectors of A
ev=eigen(A)
eigenvectors=ev$vectors
eigenvector1=ev$vectors[,1]
eigenvector2=ev$vectors[,2]
plot(X[,1],X[,2])
par(new=TRUE)
plot(eigenvector1,type="l",col="red",axes=FALSE)
########################### Problemset 5 ##############################
rm(list=ls())
dev.off()
cat("\014")
library(mvtnorm)
########################### Exercise 1 #################################
n=100
p=2
mu=0
beta=runif(p, min=0.1, max=0.5)
#covmat=diag(rnorm(p,1,1))
covmat=matrix(c(1,0.7,0.7,2),2,2)
eps=rnorm(n,0,1)
X = rmvnorm(n, mean = rep(mu, p), sigma = covmat)
Y = X %*% beta + eps
#### a)
# Step 1) Calculate cov(X)
A=cov(X)
# Step 2) Calculate Eigenvectors of A
ev=eigen(A)
eigenvectors=ev$vectors
eigenvector1=ev$vectors[,1]
eigenvector2=ev$vectors[,2]
plot(X[,1],X[,2])
par(new=TRUE)
plot(eigenvector1,type="l",col="red",axes=FALSE)
# Step 3) create Z
Z=X%*%eigenvectors
Z1=Z[,1]
Z2=Z[,2]
# Step 4) compare function to own results
q=prcomp(X)$rotation
ZT=X%*%q
#the calculated eigenvectors are the same and ZT=Z
### b)
#draw test set
X_test= rmvnorm(n, mean = rep(mu, p), sigma = covmat)
eps_test= eps=rnorm(n,0,1)
Y_test= X_test %*% beta + eps_test
X_test_d=data.frame(X_test) #predict functions need dataframe, but the one
Y_test_d=data.frame(Y_test) #for ridge does not
#perform PCR on Z1 and then Z1+Z2
PCR1=lm(Y ~ Z1)
PCR1_predict=data.frame(predict(PCR1, newdata = X_test_d))
Test_error_PCR1=(1/n)*sum((Y_test-PCR1_predict)^2)
PCR2=lm(Y ~ Z1 + Z2)
PCR2_predict=data.frame(predict(PCR2, newdata = X_test_d))
Test_error_PCR2=(1/n)*sum((Y_test-PCR2_predict)^2)
#Ridge
library(glmnet)
grid = 10^seq(10,-2, length =100)
ridge.mod = glmnet(X,Y,alpha=0, lambda=grid)
set.seed(1)
cv.out=cv.glmnet(X,Y,alpha=0)
plot(cv.out)
bestlam =cv.out$lambda.min
ridge.pred=predict(ridge.mod ,s=bestlam ,newx=X_test)
Test_error_ridge=(1/n)*sum((ridge.pred - Y_test)^2)
#OLS
OLS=lm(Y ~ X)
OLS_predict=data.frame(predict(OLS, newdata = X_test_d))
Test_error_OLS=(1/n)*sum((Y_test-OLS_predict)^2)
### extension to try differnt parameters and compare average MSE with increasing number of DGP runs
n=20
p=50
mu=0
beta=runif(p, min=0.1, max=0.5)
MCN=100
MSE=matrix(NaN,4,MCN)
for (i in 1:MCN){
#generate training set
beta=runif(p, min=1, max=5)
beta[1:p]=0
#a=runif(1,min=-0.0,max=0.0)
#covmat=matrix(c(1,a,a,2),2,2)
#temp=runif(p, min = 1, max = 1)
covmat=diag(temp)
#covmat[1:p,1:p]=1
covmat[1,1]=4
eps=rnorm(n,0,1)
X = rmvnorm(n, mean = rep(mu, p), sigma = covmat)
Y = X %*% beta + eps
#generate test set
X_test= rmvnorm(n, mean = rep(mu, p), sigma = covmat)
eps_test= eps=rnorm(n,0,1)
Y_test= X_test %*% beta + eps_test
X_test_d=data.frame(X_test)
Y_test_d=data.frame(Y_test)
#calculate Z using PCA
q=prcomp(X)$rotation
Z=X%*%q
Z1=Z[,1]
Z2=Z[,2]
#do PCR
PCR1=lm(Y ~ Z1)
PCR1_predict=data.frame(predict(PCR1, newdata = X_test_d))
Test_error_PCR1=(1/n)*sum((Y_test-PCR1_predict)^2)
PCR2=lm(Y ~ Z1 + Z2)
PCR2_predict=data.frame(predict(PCR2, newdata = X_test_d))
Test_error_PCR2=(1/n)*sum((Y_test-PCR2_predict)^2)
#Ridge
ridge.mod = glmnet(X,Y,alpha=0)
cv.out=cv.glmnet(X,Y,alpha=0)
bestlam =cv.out$lambda.min
ridge.pred=predict(ridge.mod ,s=bestlam ,newx=X_test)
Test_error_ridge=(1/n)*sum((ridge.pred - Y_test)^2)
#OLS
OLS=lm(Y ~ X)
OLS_predict=data.frame(predict(OLS, newdata = X_test_d))
Test_error_OLS=(1/n)*sum((Y_test-OLS_predict)^2)
#save results for each run
MSE[1,i]=Test_error_OLS
MSE[2,i]=Test_error_PCR1
MSE[3,i]=Test_error_PCR2
MSE[4,i]=Test_error_ridge
}
MSE_ave=matrix(NaN,4,MCN)
for (l in 1:MCN){
MSE_ave[1,l]=mean(MSE[1,1:(l)])
MSE_ave[2,l]=mean(MSE[2,1:(l)])
MSE_ave[3,l]=mean(MSE[3,1:(l)])
MSE_ave[4,l]=mean(MSE[4,1:(l)])
}
#MSE_ave[1,1:MCN]=1
plot(MSE_ave[2,],type="l",col="green",ylim=c(min(MSE_ave),max(MSE_ave)),xlim=c(0,100),axis=FALSE,
main="PCR 1, PCR 2, Ridge",ylab="average MSE", xlab="number of runs")
par(new=TRUE)
plot(MSE_ave[3,],type="l",col="blue",ylim=c(min(MSE_ave),max(MSE_ave)),xlim=c(0,100),axis=FALSE,
ylab="average MSE", xlab="number of runs")
par(new=TRUE)
plot(MSE_ave[4,],type="l",col="red",ylim=c(min(MSE_ave),max(MSE_ave)),xlim=c(0,100),axis=FALSE,
ylab="average MSE", xlab="number of runs")
par(new=TRUE)
plot(MSE_ave[1,],type="l",col="black",ylim=c(min(MSE_ave),max(MSE_ave)),xlim=c(0,100),axis=FALSE,
ylab="average MSE", xlab="number of runs")
legend(40, max(MSE_ave), legend=c("PCR 1", "PCR 2","Ridge", "OLS"),
col=c("green", "blue","red","black"), lty=1, cex=0.8)
########################### Problemset 5 ##############################
rm(list=ls())
dev.off()
cat("\014")
library(mvtnorm)
########################### Exercise 1 #################################
n=100
p=2
mu=0
beta=runif(p, min=0.1, max=0.5)
#covmat=diag(rnorm(p,1,1))
covmat=matrix(c(1,0.7,0.7,2),2,2)
eps=rnorm(n,0,1)
X = rmvnorm(n, mean = rep(mu, p), sigma = covmat)
Y = X %*% beta + eps
#### a)
# Step 1) Calculate cov(X)
A=cov(X)
# Step 2) Calculate Eigenvectors of A
ev=eigen(A)
eigenvectors=ev$vectors
eigenvector1=ev$vectors[,1]
eigenvector2=ev$vectors[,2]
plot(X[,1],X[,2])
par(new=TRUE)
plot(eigenvector1,type="l",col="red",axes=FALSE)
# Step 3) create Z
Z=X%*%eigenvectors
Z1=Z[,1]
Z2=Z[,2]
# Step 4) compare function to own results
q=prcomp(X)$rotation
ZT=X%*%q
#the calculated eigenvectors are the same and ZT=Z
#####################################################################
###   Group 10 - ProblemSet 06
#####################################################################
### Preface
# clearing workspace and set wd
rm(list=ls())
cat("\014")
dev.off()
#library(plm)        # Panel Methods - regression models
library(mvtnorm)     # random draws from a multivariate normal distr.
#library(MASS)       # to fit LDA and QDA analysis
#library(glmnet)     # Lasso and Ridge regression
library(boot)        # Bootstrap
# Note: packages above require the data to be saved as a data frame
# data config
N  = 500
p  = 4
mu = 0
## DGP
# Variance-Covariance Matrix, beta vector
set.seed(123)
temp=abs(rnorm(p, 0, 1^0.5))
covmat=diag(temp)
#beta=runif(p, min = 0.1, max = 0.5)
beta=c(1,1.5,-1.5,1.5)
# sampling data and generate y's
X = rmvnorm(N, mean = rep(mu, p), sigma = covmat)
#con = rep(1,N)
#X   = cbind(con,X)
eps = rnorm(N, 0, 1^0.5)
Y = beta[1]*X[,1] + beta[2]*(X[,2]) + beta[3]*(X[,3]^2) + beta[4]*(X[,4]^3) + eps
rm(temp, eps)
df=as.data.frame(cbind(Y,X))
## a) computing SE and CI by relying on 2*SE
model = df$Y ~ df$V3 + df$V4^2 + df$V5^3
ols = lm(model)
rslt  = summary(ols)
se    = rslt$residuals
left  = mean(Y)+se
right = mean(Y)-se
CI    = c(right,left)
#par(mfrow=c(1,2))
plot(CI)
#####################################################################
###   Group 10 - ProblemSet 06
#####################################################################
### Preface
# clearing workspace and set wd
rm(list=ls())
cat("\014")
dev.off()
#library(plm)        # Panel Methods - regression models
#library(mvtnorm)     # random draws from a multivariate normal distr.
#library(MASS)       # to fit LDA and QDA analysis
#library(glmnet)     # Lasso and Ridge regression
library(boot)        # Bootstrap
# Note: packages above require the data to be saved as a data frame
# data config
N  = 500
mu = 0
covmat = 1
## DGP
# Variance-Covariance Matrix, beta vector
set.seed(123)
beta=c(1,1.5,-1.5,1.5)
# sampling data and generate y's
X = rnorm(N, mu, covmat)
#con = rep(1,N)
#X   = cbind(con,X)
eps = rnorm(N, 0, 1^0.5)
Y = beta[1] + beta[2]*X + beta[3]*(X^2) + beta[4]*(X^3) + eps
rm(eps)
df=as.data.frame(cbind(Y,X))
#####################################################################
###   Exercise 1
## a) computing SE and CI by relying on 2*SE
model = df$Y ~ df$X + (df$X^2) + (df$X^3)
ols = lm(model)
rslt  = summary(ols)
se    = rslt$residuals
estfit= predict(ols, df)
upper  = estfit+2*se
lower = estfit-2*se
# sorting values
Y=sort(Y)
X=sort(X)
L=sort(lower)
U=sort(upper)
#par(mfrow=c(1,2))
plot(X,Y)
lines(X, L, col="red",lty=2)
lines(X, U, col="red",lty=2)
## b) computing Bootstraped CI's
bootfunc <- function(data, index) {
coefficients(lm(Y ~ X+(X^2)+(X^3) , data = data, subset = index))
}
set.seed(1)
bootstrap = boot(df, bootfunc, 1000)
boot.ci(bootstrap)
plot(bootstrap)
bootcoef = bootstrap$t0
estfit = bootcoef[1] + bootcoef[2]*X + bootcoef[3]*X^2 + bootcoef[4]*X^3
#se     =
upper  = estfit+2*se
lower = estfit-2*se
# sorting values
Y=sort(Y)
X=sort(X)
L=sort(lower)
U=sort(upper)
par(mfrow=c(1,2))
plot(X,Y)
lines(X, L, col="red",lty=2)
lines(X, U, col="red",lty=2)
getCI <- function(B, muH0, sdH0, N) {
getM <- function(orgDV, idx) {
bsM   <- mean(orgDV[idx])                       # M*
bsS2M <- (((N-1) / N) * var(orgDV[idx])) / N    # S^2*(M)
c(bsM, bsS2M)
}
DV  <- rnorm(N, muH0, sdH0)            # simulated data: original sample
M   <- mean(DV)                        # M from original sample
S2M <- (((N-1)/N) * var(DV)) / N       # S^2(M) from original sample
# bootstrap
boots   <- t(replicate(B, getM(DV, sample(seq(along=DV), replace=TRUE))))
Mstar   <- boots[ , 1]                 # M* for each replicate
S2Mstar <- boots[ , 2]                 # S^2*(M) for each replicate
biasM   <- mean(Mstar) - M             # bias of estimator M
# indices for sorted vector of estimates
idx   <- trunc((B + 1) * c(0.05/2, 1 - 0.05/2))
zCrit <- qnorm(c(1 - 0.05/2, 0.05/2))  # z-quantiles from std-normal distribution
tStar <- (Mstar-M) / sqrt(S2Mstar)     # t*
tCrit <- sort(tStar)[rev(idx)]         # t-quantiles from empirical t* distribution
ciBasic <- 2*M - sort(Mstar)[rev(idx)] # basic CI
ciPerc  <- sort(Mstar)[idx]            # percentile CI
ciNorm  <- M-biasM - zCrit*sd(Mstar)   # normal CI
ciT     <- M - tCrit * sqrt(S2M)       # studentized t-CI
c(basic=ciBasic, percentile=ciPerc, normal=ciNorm, t=ciT)
}
## 1000 bootstraps - this will take a while
B    <- 999                  # number of replicates
muH0 <- 100                  # for generating data: true mean
sdH0 <- 40                   # for generating data: true sd
N    <- 200                  # sample size
DV   <- rnorm(N, muH0, sdH0) # simulated data: original sample
Nrep <- 1000                 # number of bootstraps
CIs  <- t(replicate(Nrep, getCI(B=B, muH0=muH0, sdH0=sdH0, N=N)))
## coverage probabilities
sum((CIs[ , "basic1"]      < muH0) & (CIs[ , "basic2"]      > muH0)) / Nrep
sum((CIs[ , "percentile1"] < muH0) & (CIs[ , "percentile2"] > muH0)) / Nrep
sum((CIs[ , "normal1"]     < muH0) & (CIs[ , "normal2"]     > muH0)) / Nrep
sum((CIs[ , "t1"]          < muH0) & (CIs[ , "t2"]          > muH0)) / Nrep
plot(CIs)
View(CIs)
DV
#####################################################################
###   Group 10 - ProblemSet 06
#####################################################################
### Preface
# clearing workspace and set wd
rm(list=ls())
cat("\014")
dev.off()
library(boot)        # Bootstrap
# Note: packages above require the data to be saved as a data frame
## DGP
#Simulating OLS samples and plot the regression lines
set.seed(123)
#Two explanatory variables plus an intercept:
N         <- 200 # Number of observations
X.1       <- rep(1, N)
X.2      <- rnorm(N, mean=0, sd=1) # (pseudo) random numbers form a normal distr
X         <- cbind(X.1, X.2,X.2^2,X.2^3)
#Homoscedastic error term
eps       <-rnorm(N,0,10)#
beta.vec  <- c(1,1.5,-1.5,1.5)
y         <- X %*% beta.vec + eps
#####################################################################
###   Exercise 1
## a) computing SE and CI by relying on 2*SE
#Solving for beta hat
beta.hat <- solve(t(X) %*% X) %*% t(X) %*% y
beta.hat
x.grid<-sort(X.2) #
true_model<-beta.vec[1]+beta.vec[2]*x.grid+beta.vec[3]*x.grid^2+beta.vec[4]*x.grid^3
estim_model<-beta.hat[1]+beta.hat[2]*x.grid+beta.hat[3]*x.grid^2+beta.hat[4]*x.grid^3
plot(estim_model,type="l")
lines(true_model,type="l",lty=1,col="red")
y.hat<- X %*% beta.hat
eps.hat<-y-X %*% beta.hat
se<-(t(eps.hat)%*%(eps.hat))/(N-5)
cov<-se[1]*solve(t(X) %*% X)
new_data<-X[order(X[,2]),]
var<-c()
for(i in 1:N)
{
var[i]=t(new_data[i,])%*%cov%*%(new_data[i,])
}
d<-2*(sqrt(var))
estim<-beta.hat[1]+beta.hat[2]*new_data[,2]+beta.hat[3]*new_data[,2]^2+beta.hat[4]*new_data[,2]^3
conf_low<-estim-d
conf_high<-estim+d
plot(estim,type="l")
lines(conf_low,lty=2,col="red")
lines(conf_high,lty=2,col="blue")
## b) computing Bootstraped CI's
B=200
b_sample<-matrix(NA,N,B)
beta.hat<-matrix(NA,length(X),B)
data<-cbind(y,X)
function_B<-matrix(NA,length(x.grid),B)
for(i in 1:B)
{
XB<- data[sample(nrow(data),replace=T, N), ]
beta.hat<-solve(t(XB[,2:5]) %*% XB[,2:5]) %*% t(XB[,2:5]) %*%XB[,1]
function_B[,i]<-beta.hat[1]+beta.hat[2]*x.grid+beta.hat[3]*x.grid^2+beta.hat[4]*x.grid^3
}
plot(function_B[,1])
plot(function_B[,2])
sorted<-t(apply(function_B,1,sort))
#Calculate the empirical quantiles
conf_highB<-sorted[,195]
conf_lowB<-sorted[,5]
plot(estim,type="l",lty=3)
lines(conf_highB)
lines(conf_lowB)
lines(conf_low,lty=2,col="red")
lines(conf_high,lty=2,col="blue")
lines(true_model,col="green")
## c) computing Coverage Probabilities
cov_prob_B<-true_model>conf_lowB&true_model<conf_highB
sum(cov_prob_B)/length(x.grid)
cov_prob_A<-true_model>conf_low&true_model<conf_high
sum(cov_prob_A)/length(x.grid)
## d) comupting interval length
int_lengthB<-conf_highB-conf_lowB
int_length<-conf_high-conf_low
sum(int_lengthB/int_length>1) #counts the number of times the bootstrapped interval length is larger than the analytical one
# -*- coding: utf-8 -*-
###############################################################################
#       Preface
###############################################################################
rm(list=ls())
setwd('/Users/Fabian/Google Drive/UniBonn/X_Microeconometrics/student-project-fbalensiefer')
#setwd('C:/Users/fabia/Google Drive/UniBonn/X_Microeconometrics/student-project-fbalensiefer')
# preface loading packages required for R
library(plm)
library(lfe)
#library(haven)
library(dplyr)
library(dummies)
library(data.table)
#library(fastDummies)
###############################################################################
#       Two-way FE function
###############################################################################
TwoWayFE <- function(y, Xs, fe1, fe2, dt, cluster = FALSE) {
pkgs <- c("data.table", "RcppEigen")
sapply(pkgs, require, character.only = TRUE)
keep_cols <- c(y, Xs, fe1, fe2)
model_dt <- dt[, keep_cols, with = FALSE]; rm(keep_cols)
model_dt <- na.omit(model_dt)
num_Xs <- length(Xs)
new_names <- c("y", paste0("x", 1:num_Xs), "fe1", "fe2")
setnames(model_dt, 1:ncol(model_dt), new_names)
# Sample Means:
cols <- new_names[!grepl("fe", new_names)]
model_dt[, paste0("mean_", cols) :=
lapply(.SD, mean, na.rm = TRUE), .SDcols = cols]
# Means by FE1:
setkey(model_dt, fe1)
model_dt[,
paste0("mean_", cols, "_fe1") :=
lapply(.SD, mean, na.rm = TRUE), .SDcols = cols, by = fe1]
M <- length(unique(model_dt$fe1))
# Means by FE2:
setkey(model_dt, fe2)
model_dt[,
paste0("mean_", cols, "_fe2") :=
lapply(.SD, mean, na.rm = TRUE), .SDcols = cols, by = fe2]
Y <- length(unique(model_dt$fe2))
# Demeaning:
model_dt[, "y_tilde" := y - mean_y_fe2 - mean_y_fe1 + mean_y]
g <- function(i) {paste0("x",i,"_tilde")}
LHS <- sapply(1:num_Xs, g)
f <- function(i) {
paste0("x",i," - mean_x",i,"_fe2 - mean_x",i,"_fe1 + mean_x", i)
}
RHS <- paste0("list(",paste(sapply(1:num_Xs, f), collapse = ", "), ")")
model_dt[, eval(LHS) := eval(parse(text = RHS))]
x_cols <- grep("x\\d{1}_tilde", names(model_dt), value = TRUE)
model_dt <- model_dt[, c("y_tilde", eval(x_cols), "fe1", "fe2"),
with = FALSE]
y <- model_dt$y_tilde
X <- model_dt[, x_cols, with = FALSE]
cluster_vec <- model_dt$fe1
rm(model_dt)
m <- RcppEigen::fastLm(X, y)
names(m$coefficients) <- Xs
##############################################
DoF <- m$df.residual - (M - 1) - (Y - 1) - num_Xs + 1
# No intercept in model.
# SEs:
if(cluster){
N <- length(cluster_vec)
K <- m$rank + (M - 1) + (Y - 1)  + 1
dfc <- (M/(M - 1)) * ((N - 1)/(N - K))
est_fun <- residuals(m) * X
dt <- data.table(est_fun, fe1 = cluster_vec)
setkey(dt, fe1)
dt <- dt[, lapply(.SD, sum), by = fe1][, 2:ncol(dt), with = FALSE]
bread <- solve(crossprod(as.matrix(X))) * N
meat <- crossprod(as.matrix(dt)) / N
m$se <- as.numeric(sqrt(dfc * 1/N * diag(bread %*% meat %*% bread)))
message("SEs Clustered on First FE")
} else {
m$se <- m$se * sqrt(m$df.residual / DoF)
message("SEs Not Clustered")
}
# Correcting degrees of freedom:
m$df.residual <- DoF
return(m)
}
###############################################################################
###         Main Results
###############################################################################
# Table 6: First-Stage and Reduced-Form estimates
df = read.csv("test_csv.csv")
df = distinct(df)
