{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microeconometrics Project - Are Credit Markets Still Local? Evidence from Bank Branch Closings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Scope of this Project is to replicate a the study **\"Are Credit Markets Still Local? Evidence from Bank Branch Closings.\"** written by **Hoai-Luu Q. Nguyen**  published in *AMERICAN ECONOMIC JOURNAL: APPLIED ECONOMICS VOL. 11, NO. 1, JANUARY 2019*. <br> <br>\n",
    "\n",
    "Data and Stata-files are provided by the American Economic Association:<br>\n",
    "<href>https://www.aeaweb.org/articles?id=10.1257/app.20170543</href><br>\n",
    "\n",
    "\n",
    "\n",
    "**Hyothesis:** Does the distance to bank branches effect credit allocation?<br>\n",
    "\n",
    "**Identification Issue:** Openings and closings of bank branches are not random assignments<br>\n",
    "\n",
    "**Idea:** Using the impact of post-merger branch closings to measure the effect on lending <br>\n",
    "          => *Key assumption:* merger decision is exogenous to local economic conditions (census tract)\n",
    "          \n",
    "**Data:**\n",
    "\n",
    "        * census tract -> macro- and household data on tract level\n",
    "        * Summary of Deposits -> branch data\n",
    "        * Report of Changes -> merger and branch closing \n",
    "        * HMDA and CRA -> lending data\n",
    "        \n",
    "**Method:** \n",
    "\n",
    "        1. IV – “exposure to post-merger consolidation” as instrument for closings\n",
    "\t    2. DiD – to compare lending in exposed and control (census) tracts in the same county \n",
    "\n",
    "*Why does the author use two methods? - to allow for heterogeneity across tracts within a county (DiD)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "*This paper studies whether distance shapes credit allocation by estimating the impact of bank branch closings during the 2000s on local access to credit. To generate plausibly exogenous variation in the incidence of closings, I use an instrument based on within-county, tract-level variation in exposure to post-merger branch consolidation. Closings lead to a persistent decline in local small business lending. Annual originations fall by 453K USD after a closing, off a baseline of 4.7 million USD, and remain depressed for up to 6 years. The effects are very localized, dissipating within 6 miles, and are especially severe during the financial crisis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Methodology\n",
    "The Author uses three main data sources. First, as the main source she uses the *census tract* published by the US Census Bureau. This tracts vary in size across different regions, while containing 4000 inhabitans. <br>\n",
    "Second as to instrument exposure, she uses the data from the FDIC called *Summary of Deposits (SOD)*. This data set provides data on branches e.g. location, deposists, latitude and longitude. <br> \n",
    "Finally, data on merger activity and branch closings is provided by FDIC *Report of Changes*. Further data on loans and lenders is provided by the FFIEC *Home Mortgage Disclosure Act (HMDA)* and *Community Reinvestment Act (CRA)*<br>\n",
    "Macroeconomic data is used from the *National Establishment Time-Series (NETS)* by Walls and Associates.<br> \n",
    "Datasets are merged on bank- and tract-level by using **GIS** software to map locations. \n",
    "In the end the sample consists of tracts based on exposure to large bank mergers. Data are for the 1999-2012 period. <br> <br>\n",
    "\n",
    "In the first step the author tests whether branch closing affects local credit supply.\n",
    "Since we can not rule out potential simultaneity issues between branch closing and credit supply, the athor uses an IV approach.\n",
    "$$\\text{Close}_{it} = \\kappa_i + \\psi_t + \\rho X_{it} + \\beta_e \\text{Expose}_{it} + \\omega_{it}$$\n",
    "$$ y_{it} = \\alpha_i + \\gamma_t + \\lambda X_{it} + \\beta_c \\text{Close}_{it} + \\epsilon_{it} $$\n",
    "Further to address the issue that the decision for a merger is not turely exogenous Nguyen expands her anaylsis by an Difference-in-Differences Approach. To compare tracts who experienced a merger and tracts without (treated and control tracts/groups) within a county. \n",
    "$$y_{icmt} = \\alpha_i + (\\gamma_t \\times \\sigma_c) + X_i \\beta_t + \\sum_{\\tau} \\delta_{\\tau} (D_{mt}^{\\tau} \\times \\text{Expose}_{icm}) + \\epsilon_{icmt}$$\n",
    "Where tract $i$ in county $c$ experienced merger $m$ in year $t$. $D_{mt}^{\\tau}$ is a dummy variable equals one in year $t$ and $\\tau$ years after merger $m$ is approved. The reduced model is independent of $\\tau$, thus:\n",
    "$$y_{icmt} = \\alpha_i + (\\gamma_t \\times \\sigma_c) + X_i \\beta_t + \\delta_{\\text{POST}} (\\text{POST}_{mt} \\times \\text{Closure}_{icm}) + \\epsilon_{icmt}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Graph and Identification Issues\n",
    "\n",
    "### Causal Graph \n",
    "\n",
    "![](graphs/causal_graph.png)\n",
    "\n",
    "    * D - treatment variable \"bank branch closings\"\n",
    "    * Y - dependend variable \"lending activity\"\n",
    "    * X - bank specific controls\n",
    "    * E - general economic controls\n",
    "    * L - local economic controls\n",
    "    * M - instrument \"merger activity\"\n",
    "    * U - unobserved drivers of lending activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification \n",
    "The causal graph above pins down the relationships between banking and lending. There are multiple collider varibales, backdoorpaths, confounding varibales and reverse causality issues need to be solved to show causality. \n",
    "\n",
    "First consider our variable of interest, lending activiy. Since credit is an equilibirum concept, \n",
    "its very difficult to disentangle whether a change in lending activity is diven by a change in credit demand or supply. Second our treatment variable, bank branch closings, has an issue of reverse causality with lending activity. One can argue, that less demand for credit is affecting banks decision to close a branch in a certain location. While, we are testing whether the closing of a branch in a certain location affects credit availibility. This issues, can be solved by instrumenting bank branch closing with merger activity. The author argues, that the decision to close a branch after a merger is more driven by merger activity less then by local demand of lending. By controlling for bankspecific characteristics and general economic conditions the backdoor paths over $X$ and $E$ are blocked. Thus, the exogeneity and relevance conditions should be fullfilled.\n",
    "\n",
    "Finally, two backdoorpaths need to be blocked. Which is done by controlling for general- ($E$) and local economic ($L$) conditions.\n",
    "\n",
    "Since tracts and counties are differ in various characteristics, a concern on heterogeneity across tracts arises. The IV-approach is not able to control for such unobserved time invariante individual tract characteristics. Therefore the author expands the analysis by a difference in differences approach (DiD). This panel data method allows to account for heterogeneity. The idea is to compair treated and non-treated tracts within a county, while controlling for tract fixed-effects. General economic conditions within a county should be compairable, such that heterogeneity is not driving the results anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "%clear\n",
    "%load_ext autoreload\n",
    "\n",
    "# preface loading packages required for Python Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset provided by AEA and inspect data structure\n",
    "#df = pd.read_stata('data/replication_input.dta')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for missing values\n",
    "#df.isnull().sum()\n",
    "#df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some summary statistics\n",
    "#df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list variables\n",
    "#list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot to have a first impact\n",
    "#plt.scatter(df.num_closings, df.pmortgage)\n",
    "#plt.xlabel('number of branch closings')\n",
    "#plt.ylabel('fraction of HH with a mortgages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication of Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Buyer                                             Target  Year approved\n",
      " Manufacturers and Traders Trust Company                                      Allfirst Bank           2003\n",
      "   Bank of America, National Association                                Fleet National Bank           2004\n",
      "                      National City Bank                                 The Provident Bank           2004\n",
      "                            Regions Bank          Union Planters Bank, National Association           2004\n",
      "                     JPMorgan Chase Bank                     Bank One, National Association           2004\n",
      "                         North Fork Bank                                    GreenPoint Bank           2004\n",
      "                           SunTrust Bank                          National Bank of Commerce           2004\n",
      "     Wachovia Bank, National Association                                    SouthTrust Bank           2004\n",
      "                          Sovereign Bank                        Independence Community Bank           2006\n",
      "                            Regions Bank                                       AmSouth Bank           2006\n",
      "   Bank of America, National Association  United States Trust Company , National Associa...           2007\n",
      "            The Huntington National Bank                                           Sky Bank           2007\n",
      "   Bank of America, National Association                  LaSalle Bank National Association           2007\n"
     ]
    }
   ],
   "source": [
    "# Table 1: Merger Sample\n",
    "df = pd.read_stata('data/replication_input.dta')\n",
    "df = df[['acq_instname', 'out_instname', 'yr_approve', 'approved']]\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "df = df.sort_values(by='approved')\n",
    "df = df[['acq_instname', 'out_instname', 'yr_approve']]\n",
    "pd.options.display.float_format = '{:.0f}'.format\n",
    "df = df.rename(index=str , columns={'acq_instname':'Buyer', 'out_instname':'Target', 'yr_approve':'Year approved'})\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Variable   Median       Min         Max\n",
      "           Total assets 81954710  25963401  1252402412\n",
      "               Branches      696       254        5569\n",
      "    States of operation        8         1          31\n",
      " Countries of operation      182        18         692\n",
      "           Total assets 25955711  10426963   245783000\n",
      "               Branches      277        28        1482\n",
      "    States of operation        6         1          13\n",
      " Countries of operation       54         7         202\n"
     ]
    }
   ],
   "source": [
    "# Table 2: Merger Sammary Statistics\n",
    "df = pd.read_stata('data/replication_input.dta')\n",
    "df = df.filter(regex='mergerID|premerger_acq|premerger_out')\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "df_t = pd.DataFrame(columns=['Variable', 'Median', 'Min', 'Max'])\n",
    "df = df.filter(regex='premerger_acq|premerger_out')\n",
    "df_t['Variable'] = ['Total assets', 'Branches', 'States of operation', 'Countries of operation', 'Total assets', 'Branches', 'States of operation', 'Countries of operation']\n",
    "df_t['Median']   = np.round(np.median(df, axis=0))\n",
    "df_t['Min']      = np.round(np.nanmin(df, axis=0))\n",
    "df_t['Max']      = np.round(np.nanmax(df, axis=0))\n",
    "print(df_t.to_string(index=False))\n",
    "#np.round(df.describe(percentiles=[.5]), 1).T # we only need min, 50% (median), max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Variable   Exposed  All other  p-value 01   Control  p-value 02\n",
      "         popdensity  2575.407   7206.310       0.000  6105.754       0.000\n",
      "             poptot  5761.404   4571.781       0.000  5387.571       0.013\n",
      "          medincome 44223.772  45451.951       0.304 52171.479       0.000\n",
      "          pminority     0.211      0.386       0.000     0.238       0.039\n",
      "           pcollege     0.309      0.256       0.000     0.341       0.002\n",
      "          pmortgage     0.688      0.708       0.014     0.721       0.000\n",
      "            pincome   114.473    101.963       0.000   118.616       0.149\n",
      " cont_totalbranches     5.849      1.141       0.000     3.816       0.000\n",
      "      cont_brgrowth     0.053      0.030       0.011     0.065       0.185\n",
      "   cont_NumSBL_Rev1   103.400     54.335       0.000    88.947       0.000\n",
      "  cont_total_origin   277.220    227.091       0.000   281.016       0.723\n",
      "                Obs   386.000  18027.000         nan  3087.000         nan\n"
     ]
    }
   ],
   "source": [
    "# todo add standard devidations in brackets\n",
    "# Table 3: Summary Statistics for Exposed and Control Tracts\n",
    "#df.assign(cntyID = df.sort_values(['state_fps', 'cnty_fps']))\n",
    "filter_cols = 'poptot|popdensity|pminority|pcollege|pincome|medincome|pmortgage|cont_totalbranches|cont_brgrowth|cont_total_origin|cont_NumSBL_Rev1|Obs'\n",
    "df = pd.read_stata('data/mergersample_controls.dta')\n",
    "index = ['popdensity','poptot','medincome','pminority','pcollege','pmortgage','pincome','cont_totalbranches', 'cont_brgrowth','cont_NumSBL_Rev1','cont_total_origin']\n",
    "df_t = pd.DataFrame(columns=['Variable', 'Exposed', 'All other', 'p-value 01', 'Control', 'p-value 02'], index=index)\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "df_exposed = df.loc[df.overlap==1]\n",
    "df_exposed=df_exposed.assign(Obs=lambda df_exposed:len(df_exposed))\n",
    "df_exposed = df_exposed.filter(regex=filter_cols).T\n",
    "df = pd.read_stata('data/mergersample_controls.dta')\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "df_all     = df.loc[df.overlap==0]\n",
    "df_all=df_all.assign(Obs=lambda df_all:len(df_all))\n",
    "df_all     = df_all.filter(regex=filter_cols).T\n",
    "df01 = pd.read_stata('data/replication_input.dta')\n",
    "df01 = df01.filter(regex='state_fps|cnty_fps|tractstring|overlap|mergerID')\n",
    "df01.drop_duplicates(keep='first', inplace=True)\n",
    "df02 = pd.read_stata('data/mergersample_controls.dta')\n",
    "df02.drop_duplicates(keep='first', inplace=True)\n",
    "#df = pd.merge(df01,df02,  on=['state_fps','cnty_fps','tractstring','mergerID'], how='inner', suffixes=('', '_y'))\n",
    "df = pd.merge(df01,df02,  on=['state_fps','cnty_fps','tractstring','mergerID'], how='inner')\n",
    "df = df.loc[df.overlap_y==0]\n",
    "df_control = df.groupby(['state_fps','cnty_fps'], as_index=True)\n",
    "#df_control = df.groupby('state_fps')\n",
    "df_control = df_control.agg(np.mean)\n",
    "df_control = df.assign(Obs=lambda df:len(df))\n",
    "df_control = df_control.filter(regex=filter_cols).T\n",
    "df = df.filter(regex=filter_cols)\n",
    "df_t['Variable']  = list(df)\n",
    "df_t              = df_t.append({'Variable':'Obs'}, ignore_index=True)\n",
    "df_t['Exposed']   = np.round(np.nanmean(df_exposed, axis=1), decimals=3)\n",
    "df_t['All other'] = np.round(np.nanmean(df_all, axis=1), decimals=3)\n",
    "ptemp = stats.ttest_ind(df_exposed, df_all, axis=1, equal_var=True, nan_policy='omit')\n",
    "df_t['p-value 01']   = np.round(np.ma.getdata(ptemp[1]), decimals=3)\n",
    "df_t['Control']   = np.round(np.nanmean(df_control, axis=1), decimals=3)\n",
    "ptemp = stats.ttest_ind(df_exposed, df_control, axis=1, equal_var=True, nan_policy='omit')\n",
    "df_t['p-value 02']   = np.round(np.ma.getdata(ptemp[1]), decimals=3)\n",
    "filter_cols = ['poptot','popdensity','pminority','pcollege','pincome','medincome','pmortgage','cont_totalbranches','cont_brgrowth','cont_total_origin','cont_NumSBL_Rev1']\n",
    "\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "print(df_t.to_string(index=False))\n",
    "#print(df_control)\n",
    "#np.shape(ptemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Variable       All  Closings    Merger\n",
      "    popdensity  4032.403  3615.265  6166.240\n",
      "        poptot  4687.897  4941.796  5401.217\n",
      "     medincome 44829.351 45248.801 51803.494\n",
      "     pminority     0.195     0.197     0.234\n",
      "      pcollege     0.247     0.272     0.337\n",
      "     pmortgage     0.668     0.684     0.716\n",
      " totalbranches     2.611     3.893     4.095\n",
      "      brgrowth     0.125     0.153     0.071\n",
      "   NumSBL_Rev1    89.156   106.479    91.880\n",
      "  total_origin   317.562   342.827   281.857\n",
      "       pincome   104.267   106.273   119.300\n",
      "           Obs 37041.000  7768.000  3003.000\n"
     ]
    }
   ],
   "source": [
    "# todo add standard devidations in brackets\n",
    "# Table 4: Representativeness of the Merger Sample\n",
    "filter_cols = 'poptot|popdensity|pminority|pcollege|pincome|medincome|pmortgage|cont_totalbranches|cont_brgrowth|cont_total_origin|cont_NumSBL_Rev1|Obs'\n",
    "df = pd.read_stata('data/alltract_controls.dta')\n",
    "#index = ['popdensity','poptot','medincome','pminority','pcollege','pmortgage','pincome','cont_totalbranches', 'cont_brgrowth','cont_NumSBL_Rev1','cont_total_origin']\n",
    "index = ['popdensity', 'poptot', 'medincome', 'pminority', 'pcollege', 'pmortgage', 'totalbranches', 'brgrowth', 'NumSBL_Rev1', 'total_origin', 'pincome', 'Obs']\n",
    "df_t = pd.DataFrame(columns=['Variable', 'All', 'Closings', 'Merger'], index=index)\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "df_all = df.loc[df['year']>=2002]\n",
    "df_all = df_all.loc[df_all['year']<=2007]\n",
    "df_all = df_all.groupby(['state_fps', 'cnty_fps', 'tractstring'])\n",
    "df_all = df_all.agg(np.nanmax)\n",
    "#df_all = df_all.agg(np.nanmean)\n",
    "df_all = df_all.loc[df_all['totalbranches']>0]\n",
    "df_all['Obs'] = len(df_all)\n",
    "df_all = df_all.agg(np.nanmean)\n",
    "alltract_index = 'popdensity|poptot|medincome|pminority|pcollege|pmortgage|totalbranches|brgrowth|NumSBL_Rev1|total_origin|pincome|Obs'\n",
    "df_all = df_all.filter(regex=alltract_index)\n",
    "\n",
    "df_closing = df.loc[df['year']>=2002]\n",
    "df_closing = df_closing.loc[df_closing['year']<=2007]\n",
    "df_closing = df_closing.groupby(['state_fps', 'cnty_fps', 'tractstring'])\n",
    "df_closing = df_closing.agg(np.nanmax)\n",
    "df_closing = df_closing.loc[df_closing['num_closings']>0]\n",
    "df_closing['Obs'] = len(df_closing)\n",
    "df_closing = df_closing.agg(np.nanmean)\n",
    "df_closing = df_closing.filter(regex=alltract_index).T\n",
    "\n",
    "df = pd.read_stata('data/replication_input.dta')\n",
    "#df_merger      = df.loc[df['year']==2001]\n",
    "df_merger = df.loc[df['year']>=2002]\n",
    "df_merger = df_merger.loc[df_merger['year']<=2007]\n",
    "df01 = pd.read_stata('data/alltract_controls.dta')\n",
    "df01.drop_duplicates(keep='first', inplace=True)\n",
    "df_merger = pd.merge(df_merger,df01,  on=['state_fps','cnty_fps','tractstring'], how='inner', suffixes=('', '_y'))\n",
    "df_merger = df_merger.groupby(['state_fps', 'cnty_fps', 'tractstring'])\n",
    "df_merger = df_merger.agg(np.nanmax)\n",
    "num= len(df_merger)\n",
    "df_merger = df_merger.agg(np.nanmean)\n",
    "df_merger.rename(index= {'cont_totalbranches': 'totalbranches', 'cont_brgrowth':'brgrowth', 'cont_NumSBL_Rev1':'NumSBL_Rev1','cont_total_origin':'total_origin'},  inplace = True)\n",
    "df_merger = df_merger.filter(regex=alltract_index).T\n",
    "df_merger = df_merger.iloc[4:15]\n",
    "df_merger['Obs'] = num\n",
    "\n",
    "df_t['Variable']  = list(index)\n",
    "df_t['All']       = np.round(df_all, decimals=3)\n",
    "df_t['Closings']  = np.round(df_closing, decimals=3)\n",
    "df_t['Merger']    = np.round(df_merger, decimals=3)\n",
    "\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "print(df_t.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Variables  ecomp  ratio\n",
      "         popdensity 18.308  0.366\n",
      "             poptot 57.533  1.151\n",
      "          medincome 29.087  0.582\n",
      "          pminority 60.262  1.205\n",
      "           pcollege 47.011  0.940\n",
      "          pmortgage 39.219  0.784\n",
      "            pincome 41.426  0.829\n",
      " cont_totalbranches 86.497  1.730\n",
      "      cont_brgrowth 49.612  0.992\n",
      "   cont_NumSBL_Rev1 60.650  1.213\n",
      "  cont_total_origin 48.051  0.961\n"
     ]
    }
   ],
   "source": [
    "# Table 5: Complier Characteristics\n",
    "\n",
    "## calculating MEDIAN values\n",
    "df = pd.read_stata('data/replication_input.dta')\n",
    "index = ['popdensity','poptot','medincome','pminority','pcollege','pmortgage','pincome','cont_totalbranches', 'cont_brgrowth','cont_NumSBL_Rev1','cont_total_origin']\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "df=df.assign(event_year=lambda df:df.year-df.yr_approve)\n",
    "df=df.loc[df['event_year']==1]\n",
    "p50=df[index].median()\n",
    "\n",
    "## estimate the proportion of ALWAYS TAKERS, NEVER TAKERS and COMPLIERS\n",
    "df_at = df.loc[df['overlap']==0]\n",
    "p_always = np.nanmean(df_at.closed_branch)\n",
    "df_nt = df.assign(temp=lambda df:1-df.closed_branch)\n",
    "df_nt = df_nt.loc[df_nt['overlap']==1]\n",
    "p_never = np.nanmean(df_nt.temp)\n",
    "p_comp=1-p_always-p_never\n",
    "\n",
    "## ESTIMATE AVERAGE CHARACTERISTICS OVER SET OF ALWAYS TAKERS AND COMPLIERS COMBINED (i.e.,Treatment tracts who had closings)\n",
    "df_t1 = pd.DataFrame(index=index)\n",
    "df_atcomp = df.loc[df['overlap']==1]\n",
    "df_atcomp = df_atcomp.loc[df_atcomp['closed_branch']==1]\n",
    "n=len(df_atcomp)\n",
    "for i in index:\n",
    "    temp=df_atcomp[df_atcomp[i]>p50[i]].count()\n",
    "    df_t1[i]=temp/n\n",
    "df_t1=df_t1.T\n",
    "df_atcomp=df_t1['poptot']\n",
    "\n",
    "## ESTIMATE AVERAGE CHARACTERISTICS OVER ALWAYS TAKERS ONLY (i.e., Control tracts who had closings)\n",
    "df_t2 = pd.DataFrame(index=index)\n",
    "df_at = df.loc[df['overlap']==0]\n",
    "df_at = df_at.loc[df_at['closed_branch']==1]\n",
    "n=len(df_at)\n",
    "for i in index:\n",
    "    temp=df_at[df_at[i]>p50[i]].count()\n",
    "    df_t2[i]=temp/n\n",
    "df_t2=df_t2.T\n",
    "df_at=df_t2['poptot']\n",
    "\n",
    "##  ESTIMATE AVERAGE CHARACTERISTICS FOR COMPLIERS\n",
    "ecomp=((p_always+p_comp)/p_comp)*(df_atcomp-((p_always/(p_always+p_comp))*df_at))\n",
    "## CALCULATE RATIO\n",
    "ratio=ecomp/0.5\n",
    "## PRINT to Table\n",
    "df_tab = pd.DataFrame(columns=['Variables','ecomp','ratio'], index=index)\n",
    "df_tab['Variables']=list(df[index])\n",
    "df_tab['ecomp']=ecomp*100\n",
    "df_tab['ratio']=ratio\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "print(df_tab.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication of the Main Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
