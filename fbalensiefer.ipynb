{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microeconometrics Project by Fabian Balensiefer - 'Are Credit Markets Still Local? Evidence from Bank Branch Closings'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference\n",
    "Scope of this project is to replicate the study **\"Are Credit Markets Still Local? Evidence from Bank Branch Closings.\"** written by **Hoai-Luu Q. Nguyen**  published in *AMERICAN ECONOMIC JOURNAL: APPLIED ECONOMICS VOL. 11, NO. 1, JANUARY 2019*. <br> <br>\n",
    "\n",
    "Data and stata-files are provided by the American Economic Association:<br>\n",
    "<href>https://www.aeaweb.org/articles?id=10.1257/app.20170543</href><br>\n",
    "\n",
    "\n",
    "\n",
    "**Hyothesis:** Does the distance to bank branches effect credit allocation?<br>\n",
    "\n",
    "**Identification Issue:** Openings and closings of bank branches are not random assignments<br>\n",
    "\n",
    "**Idea:** Using the impact of post-merger branch closings to measure the effect on lending <br>\n",
    "          => *Key assumption:* merger decision is exogenous to local economic conditions (census tract)\n",
    "          \n",
    "**Data:**\n",
    "\n",
    "        * census tract -> macro- and household data on tract level\n",
    "        * Summary of Deposits -> bank branch data\n",
    "        * Report of Changes -> merger and branch closing \n",
    "        * HMDA and CRA -> lending data\n",
    "        \n",
    "**Method:** \n",
    "\n",
    "        1. IV – “exposure to post-merger consolidation” as instrument for closings\n",
    "        2. DD – to compare lending in exposed and control (census) tracts in the same county \n",
    "\n",
    "*Why does the author use two methods? - to allow for heterogeneity across tracts within a county (DD)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief summary of Nguyen (2019)\n",
    "\n",
    "The paper “Are Credit Markets Still Local? Evidence from Bank Branch Closings” by Nguyen (2019) established a novel approach for estimating the causal impact of bank branch closings on credit supply at the branch level. Motivated by the question whether technological progress have changed the access to credit she is interested in estimating the local average treatment effect of bank branch closings. While other research focuses on aggregate system wide shocks, Nguyen (2019) concentrates on a local approach. Thus, she is able to control for unobserved heterogeneity across different regions. \n",
    "\n",
    "Nguyen (2019) combines national data from four different sources. First, macro and household data on tract level is provided by the census bureau. Data on bank branches are published in the Summary of Deposits, while merger and bank branch closings are recorded in the Report of Changes (both by FDIC). Lending data for private lending/ mortgages (HMDA) and commercial lending (CRA) came from the FFIEC. Finally, some additional macroeconomic data is used from the National Establishment Time-Series (NETS) by Walls and Associates. Thus, most of the used data is provided by US official institutions. Data is merged on bank- and tract level by using GIS software to map geographical locations. Tracts are defined by the census bureau as regions containing approximately 4000 inhabitants, while differing in size. The final sample consists of tracts based on exposure to large bank mergers in the period between 1999 and 2012. \n",
    "\n",
    "The Hypothesis of the paper, whether distance to bank branches effect credit allocation, is analyzed in a quasi-experimental research design. A difference in differences framework allows to control for time-varying trends across tracts within the same county. Instrumenting bank branch closings with bank mergers addresses the endogeneity issue of bank branch closings. More about the identification will be discussed in the following section. \n",
    "\n",
    "Results of Nguyen (2019) support the hypothesis that distance to bank branches still affect access to credit. Findings suggest that “closings lead to a persistent decline in local small business lending” (Nguyen 2019). The effect on private lending seems to be of temporary nature (since this temporary decline is insignificant, we cannot infer any results).  \n",
    "Nguyen (2019) concludes that “distance matters not only because it improves accessibility, but also because it reduces the costs of transmitting information”.  This is in line with theoretical findings from Akerlof (1970) and Stiglitz and Weiss (1981), which find that credit markets are subject to informational asymmetries. Surprisingly, after major improvements in information technologies in the past decades these findings still hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Graph and Identification Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal graphs break down complex relationships into simple, transparent and easy to interpret visualizations. I create a causal graph to emphasize the papers framework. Especially, to illustrate the authors identification strategy and to discuss potential identification issues.\n",
    "\n",
    "![](graphs/causal_graph.png)\n",
    "\n",
    "    * D - treatment variable \"bank branch closings\"\n",
    "    * Y - dependent variable \"lending activity\"\n",
    "    * E - general economic controls (county level)\n",
    "    * X - local economic controls (tract level)\n",
    "    * M - instrument \"merger activity\"\n",
    "    * U - unobserved drivers of lending activity and bank branch  closings\n",
    "    \n",
    "The causal graph above pins down the relationships between banks and lending. In detail, the effect of interest in this paper is between bank branch closings (D) and lending activity (Y). There are multiple backdoor paths, confounding variables and reverse causality issues that need to be solved to show causality.\n",
    "\n",
    "First consider our variable of interest, lending activity. Since credit is an equilibrium concept, it is difficult to disentangle whether a change in lending activity is driven by a change in credit demand or supply. To solve this issue, we need exogenous variation only effecting the supply side. The author needs to identify a shock, which only effects the banks’ lending supply. Second, our treatment variable, bank branch closings, has an issue of reverse causality with lending activity. One can argue that less demand for credit is affecting banks decision to close a branch in a certain location. Therefore, we are testing whether the closing of a branch in a certain location affects credit availability. This issue can be solved by instrumenting bank branch closing with exposure to merger (M). The author argues that the decision to close a branch after a merger is more driven by merger activity than by local demand of lending. Furthermore, merger induced branch closings can be seen as exogenous shock to local credit supply. Therefore, the framework enables to disentangle between credit demand and supply. By controlling for local (X) and general (E) economic conditions the remaining two backdoor paths are blocked. Finally, there are other unobserved characteristics which could affect both lending activity and bank branch closings at the same time. By controlling for individual fixed effects on the tract level (local) the author addresses this issue. Thus, the exogeneity and relevance conditions should be fulfilled.\n",
    "\n",
    "A naive approach to measure the effect of bank branch closings on lending activity would be:\n",
    "\n",
    "$$ y_{it}=\\alpha_i + \\gamma_t + \\lambda X_{it} + \\beta_C \\text{Close}_{it} + \\epsilon_{it} $$\n",
    "\n",
    "As explained above we have an issue with reverse causality between $y_{it}$ and $ Close_{it}$.  Nguyen (2019) uses exposure to post-merger consolidation as an instrument for bank branch closings. She argues that large institutions (as in the dataset) often have overlapping networks in the same regions and therefore are at greater risk of a post-merger closings. The first stage of the instrumental variable (IV) approach has the following structure:\n",
    "\n",
    "$$ \\text{Close}_{it} = \\kappa_{i} + \\phi_{t} + \\rho X_{it} +\\text{Expose}_{it} + \\omega_{it}$$\n",
    "\n",
    "Here the crucial assumption is that the decision to merger is not affected by local economic conditions arises. The exogeneity of our instrument exposure to merger consolidation ensures a random assignment of treatment. To further address the exogeneity concern Nguyen (2019) focuses on mergers between large banks (both buyer and target held at least $10 billion in premerger assets). The idea to focus on large banks is that these institutions merge because of other factors than local economic conditions. \n",
    "\n",
    "Since tracts and counties differ in various characteristics, a concern on heterogeneity across tracts arises. The instrumental variable approach captures time-invariant tract and year specific characteristics by including individual and time fixed effects. But these fixed effects are not able to control for unobserved time-varying individual tract characteristics. Therefore, the author expands the analysis by a difference in differences approach (DD). Both panel data methods allow to account for heterogeneity. The idea is to compare treated and non-treated tracts within a county, while controlling for tract and county-by-year fixed-effects. General economic conditions are captured by county-by-year fixed-effects, such that heterogeneity is not affecting the results anymore. According to Wooldridge (2015), to evaluate the local average treatment effect (LATE), the difference in differences framework hinges on the parallel trends’ assumption. In this particular framework exposed and control tracts should evolve the same in absent of a merger (table 3 compares sample groups).\n",
    "\n",
    "So far we discussed the internal validity of this natural experiment, now focus on the external validity. Nguyen (2019) raises the question “Is the local average treatment effect (LATE) identified from merger-induced closings informative for understanding the impact of branch closings more generally?”. This is rather on the validity of the instrument, than comparison of merger sample tracts and other branched tracts. As Nguyen (2019) points out, the interpretation of the LATE is the effect of treatment on compliers. \n",
    "\n",
    "$$ \\text{LATE} = E[Y(1)-Y(0) \\mid D(1)=1, D(0)=0] $$\n",
    "\n",
    "Since it is not possible to identify compliers directly, she applies the procedure by Angrist and Pischke (2009). This procedure describes how to construct complier characteristics based on fractions of always- ($\\pi_A$) and never-takers ($\\pi_N$). \n",
    "\n",
    "$$ \\pi_C = 1-\\pi_A-\\pi_N  $$\n",
    "\n",
    "The results of the representativeness of compliers are presented in table 5.\n",
    "\n",
    "The difference in differences framework is presented as the following model, with a year-by-year estimation of treatment effect:\n",
    "\n",
    "$$y_{icmt} = \\alpha_i + (\\gamma_t \\times \\sigma_c) + X_i \\beta_t + \\sum_{\\tau} \\delta_{\\tau} (D_{mt}^{\\tau} \\times \\text{Expose}_{icm}) + \\epsilon_{icmt}$$\n",
    "\n",
    "Where tract $i$ in county $c$ experienced merger $m$ in year $t$. $D_{mt}^{\\tau}$ is a dummy variable that equals one in year $t$ and $\\tau$ years after merger $m$ is approved. The reduced model is independent of $\\tau$, thus:\n",
    "\n",
    "$$y_{icmt} = \\alpha_i + (\\gamma_t \\times \\sigma_c) + X_i \\beta_t + \\delta_{\\text{POST}} (\\text{POST}_{mt} \\times \\text{Closure}_{icm}) + \\epsilon_{icmt}$$\n",
    "\n",
    "This functional form is typically used for a difference in differences approach. It is less flexible but easy to interpret. The Dummy variables $\\text{POST}$ and $\\text{Closure}$ are interacted, thus, the interacted term is equal to one if a tract experienced closings after a merger. The results are presented in table 6 and 7.\n",
    "\n",
    "Coming back to the discussion of external validity of the identification framework, Nguyen (2019) compares tracts from the merger sample with the average branched tracts in the US. Since the difference in differences framework identifies only the effect of merger induced closings, it is questionable whether the effect is representative. According to Nguyen (2019), merger sample tracts are in general wealthier and have larger banking markets. Therefore, the LATE is likely underestimated. The simulation study later in this paper is showing why the reduced form is underestimating the true effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "%clear\n",
    "%load_ext autoreload\n",
    "\n",
    "# preface loading packages required for Python Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from auxiliary import *\n",
    "from linearmodels import PanelOLS\n",
    "from linearmodels.iv import IV2SLS\n",
    "#from statsmodels.iolib.summary2 import summary_col\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication of Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 1 shows the bank mergers Nguyen (2019) uses for her analysis. I am fully able to replicate this table, which consists of 13 bank mergers. As described before these are mergers from large banks to emphasize exogeneity of the instrument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1: Merger Sample\n",
      "\n",
      "                                   Buyer                                             Target  Year approved\n",
      " Manufacturers and Traders Trust Company                                      Allfirst Bank           2003\n",
      "   Bank of America, National Association                                Fleet National Bank           2004\n",
      "                      National City Bank                                 The Provident Bank           2004\n",
      "                            Regions Bank          Union Planters Bank, National Association           2004\n",
      "                     JPMorgan Chase Bank                     Bank One, National Association           2004\n",
      "                         North Fork Bank                                    GreenPoint Bank           2004\n",
      "                           SunTrust Bank                          National Bank of Commerce           2004\n",
      "     Wachovia Bank, National Association                                    SouthTrust Bank           2004\n",
      "                          Sovereign Bank                        Independence Community Bank           2006\n",
      "                            Regions Bank                                       AmSouth Bank           2006\n",
      "   Bank of America, National Association  United States Trust Company , National Associa...           2007\n",
      "            The Huntington National Bank                                           Sky Bank           2007\n",
      "   Bank of America, National Association                  LaSalle Bank National Association           2007\n"
     ]
    }
   ],
   "source": [
    "# Table 1: Merger Sample\n",
    "print('Table 1: Merger Sample\\n')\n",
    "print(tab1().to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 2 summarizes buyer and target characteristics prior the treatment. Again, the focus is on large banks and I am fully able to replicate table 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 2: Merger Sammary Statistics\n",
      "\n",
      "               Variable   Median       Min         Max\n",
      "           Total assets 81954710  25963401  1252402412\n",
      "               Branches      696       254        5569\n",
      "    States of operation        8         1          31\n",
      " Countries of operation      182        18         692\n",
      "           Total assets 25955711  10426963   245783000\n",
      "               Branches      277        28        1482\n",
      "    States of operation        6         1          13\n",
      " Countries of operation       54         7         202\n"
     ]
    }
   ],
   "source": [
    "# Table 2: Merger Sammary Statistics\n",
    "print('Table 2: Merger Sammary Statistics\\n')\n",
    "print(tab2().to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 3 compares the sample groups and provides summary statistics. Concentrating on column 3, exposed (treatment) tracts are similar to other tracts in the sample, while column 5 indicates some significant difference between exposed and control tracts. Nguyen (2019) argues that exposed tracts are wealthier and have larger banking markets than the average US tracts. This results in an underestimation of the estimates. First, the stata-file does not include the measures ‘Establishment growth’ and ‘Employment growth’. Due to some differences between the computation in stata and the scipy package, some of the p-values differ. One reason could be that I compute the p-value conditioned on my generated data-frame, while Nguyen (2019) estimates the p-value with a fixed effects regression. Therefore, the qualitative result still holds, there is a difference between exposed and control tracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 3: Summary Statistics for Exposed and Control Tracts\n",
      "\n",
      "Variable                         Exposed      All other        p-value        Control        p-value\n",
      "\n",
      "Population density               2575.41        7206.31          0.000        6105.75          0.000\n",
      "                                 7925.48       14576.32                      13868.57               \n",
      "Population                       5761.40        4571.78          0.000        5387.57          0.013\n",
      "                                 3229.73        2365.53                       2714.44               \n",
      "Median Income                   44223.77       45451.95          0.304       52171.48          0.000\n",
      "                                20288.25       23290.21                      24045.71               \n",
      "Fraction minority                   0.21           0.39          0.000           0.24          0.039\n",
      "                                    0.23           0.34                          0.24               \n",
      "Fraction college educated           0.31           0.26          0.000           0.34          0.002\n",
      "                                    0.19           0.19                          0.19               \n",
      "Fraction mortgage                   0.69           0.71          0.014           0.72          0.000\n",
      "                                    0.15           0.16                          0.15               \n",
      "Percent MSA median income         114.47         101.96          0.000         118.62          0.149\n",
      "                                   46.13          51.36                         53.95               \n",
      "Total branches                      5.85           1.14          0.000           3.82          0.000\n",
      "                                    3.92           1.94                          2.39               \n",
      "Branch growth                       0.05           0.03          0.011           0.07          0.185\n",
      "                                    0.13           0.17                          0.17               \n",
      "SBL originations                  103.40          54.34          0.000          88.95          0.000\n",
      "                                   53.40          44.84                         50.87               \n",
      "Mortgage originations             277.22         227.09          0.000         281.02          0.723\n",
      "                                  203.29         178.99                        188.96               \n",
      "Obs                                  386          18027                          3087               \n"
     ]
    }
   ],
   "source": [
    "# Table 3: Summary Statistics for Exposed and Control Tracts\n",
    "df_t=tab3()[0]\n",
    "std=tab3()[1]\n",
    "index=tab3()[2]\n",
    "var=pd.DataFrame(columns=['list'], index=index)\n",
    "var['list']=['Population density','Population','Median Income','Fraction minority','Fraction college educated','Fraction mortgage','Percent MSA median income','Total branches','Branch growth','SBL originations','Mortgage originations']\n",
    "print('Table 3: Summary Statistics for Exposed and Control Tracts\\n')\n",
    "print('{:<25s}{:>15s}{:>15s}{:>15s}{:>15s}{:>15s}\\n'.format('Variable','Exposed','All other','p-value','Control','p-value'))\n",
    "for i in index:\n",
    "    print('{:<25s}{:>15.2f}{:>15.2f}{:>15.3f}{:>15.2f}{:>15.3f}'.format(var.list[i], df_t.Exposed[i], df_t.Allother[i], df_t.pvalue01[i], df_t.Control[i], df_t.pvalue02[i]))\n",
    "    print('{:<25s}{:>15.2f}{:>15.2f}{:>15s}{:>15.2f}{:>15s}'.format(' ',std.a[i], std.b[i], ' ',std.c[i], ' '))\n",
    "print('{:<25s}{:>15.0f}{:>15.0f}{:>15.0s}{:>15.0f}{:>15.2s}'.format('Obs',df_t.Exposed['Obs'], df_t.Allother['Obs'], ' ', df_t.Control['Obs'], ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 4 presents the summary statistics of the merger sample. All variables are fixed to year 2001, prior any mergers. Column 1 contains all tracts with bank branch information, while column 2 contains tracts which experienced closings. Finally, column 3 presents the treatment sample containing tracts which experienced a bank merger. I am able to completely replicate table 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 4: Representativeness of the Merger Sample\n",
      "\n",
      "Variable                  All branched tracts     Tracts with closings       Merger sample\n",
      "\n",
      "Population density                    4032.40                  3615.26             6166.24\n",
      "                                     10052.64                  7348.34            14319.17\n",
      "Population                            4687.90                  4941.80             5401.22\n",
      "                                      2193.43                  2430.65             2702.60\n",
      "Median Income                        44829.35                 45248.80            51699.99\n",
      "                                     20213.36                 20685.61            23886.71\n",
      "Fraction minority                        0.20                     0.20                0.23\n",
      "                                         0.24                     0.22                0.24\n",
      "Fraction college educated                0.25                     0.27                0.34\n",
      "                                         0.17                     0.17                0.19\n",
      "Fraction mortgage                        0.67                     0.68                0.72\n",
      "                                         0.15                     0.15                0.15\n",
      "Total branches                           2.05                     3.14                3.64\n",
      "                                         1.95                     2.88                2.60\n",
      "Branch growth                            0.02                     0.04                0.04\n",
      "                                         0.14                     0.16                0.15\n",
      "SBL originations                        45.03                    58.69               70.56\n",
      "                                        36.55                    43.87               46.88\n",
      "Mortgage originations                  193.47                   210.63              238.42\n",
      "                                       156.38                   164.45              167.42\n",
      "Percent MSA median income              104.27                   106.27              118.90\n",
      "                                        38.17                    41.23               53.64\n",
      "Obs                                     37041                     7768                3003\n"
     ]
    }
   ],
   "source": [
    "# Table 4: Representativeness of the Merger Sample\n",
    "df_t=tab4()[0]\n",
    "index=tab4()[1]\n",
    "std=tab4()[2]\n",
    "var=pd.DataFrame(columns=['list'], index=index)\n",
    "var['list']=['Population density','Population','Median Income','Fraction minority','Fraction college educated','Fraction mortgage','Total branches','Branch growth','SBL originations','Mortgage originations','Percent MSA median income']\n",
    "print('Table 4: Representativeness of the Merger Sample\\n')\n",
    "print('{:<25s}{:>20s}{:>25s}{:>20s}\\n'.format('Variable','All branched tracts','Tracts with closings','Merger sample'))\n",
    "for i in index:\n",
    "     print('{:<25s}{:>20.2f}{:>25.2f}{:>20.2f}'.format(var.list[i], df_t.All[i], df_t.Closings[i], df_t.Merger[i]))\n",
    "     print('{:<25s}{:>20.2f}{:>25.2f}{:>20.2f}'.format(' ',std.a[i], std.b[i], std.c[i]))\n",
    "print('{:<25s}{:>20.0f}{:>25.0f}{:>20.0f}'.format('Obs',df_t.All['Obs'], df_t.Closings['Obs'], df_t.Merger['Obs'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 5 shows the complier characteristics using the methodology by Angrist and Pischke (2009). Column 1 contains the proportion of compliers above the sample median, while column 2 presents the complier to sample ratios. According to Nguyen (2019), compliers tend to be fairly representative of the median tract sample. Again, table 5 can fully be replicated using the data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 5: Complier Characteristics\n",
      "\n",
      "Variable                 Proportion of compliers above the sample median (percent)              Ratio: Compliers to sample\n",
      "\n",
      "Population density                                        18                                                   0.37\n",
      "Population                                                58                                                   1.15\n",
      "Median Income                                             29                                                   0.58\n",
      "Fraction minority                                         60                                                   1.21\n",
      "Fraction college educated                                 47                                                   0.94\n",
      "Fraction mortgage                                         39                                                   0.78\n",
      "Percent MSA median income                                 41                                                   0.83\n",
      "Total branches                                            86                                                   1.73\n",
      "Branch growth                                             50                                                   0.99\n",
      "SBL originations                                          61                                                   1.21\n",
      "Mortgage originations                                     48                                                   0.96\n"
     ]
    }
   ],
   "source": [
    "# Table 5: Complier Characteristics\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "df_t5=tab5()\n",
    "index=df_t5.index\n",
    "var=pd.DataFrame(columns=['list'], index=index)\n",
    "var['list']=['Population density','Population','Median Income','Fraction minority','Fraction college educated','Fraction mortgage','Percent MSA median income','Total branches','Branch growth','SBL originations','Mortgage originations']\n",
    "print('Table 5: Complier Characteristics\\n')\n",
    "#print(tab5().to_string(index=False))\n",
    "print('{:<25s}{:>20s}{:>40s}\\n'.format('Variable','Proportion of compliers above the sample median (percent)','Ratio: Compliers to sample'))\n",
    "for i in index:\n",
    "    print('{:<25s}{:>35.0f}{:>55.2f}'.format(var.list[i], df_t5.ecomp[i], df_t5.ratio[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, I am able to replicate the summary statistics using the provided data and python. Only the calculation of the p-values in table 3 differs. As mentioned above, this can be caused by a different computation/ implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication of the Main Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'drop_absorbed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-77efe28b198b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfig2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfig2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxhline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lightgrey'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'--'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxvline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lightgrey'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'--'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\UniBonn\\Z_Python_SimulationStudy\\auxiliary.py\u001b[0m in \u001b[0;36mfig2\u001b[1;34m()\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[0mexog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdummylist\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcontrollist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdummylist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPanelOLS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdftest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_closings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdftest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexog\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentity_effects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_effects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_absorbed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m     \u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcov_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'clustered'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdftest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclustID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'drop_absorbed'"
     ]
    }
   ],
   "source": [
    "mean=fig2()[0]\n",
    "std=fig2()[1]\n",
    "ind=range(-7,9)\n",
    "plt.axhline(y=0.0, color='lightgrey', linestyle='--')\n",
    "plt.axvline(x=0.0, color='lightgrey', linestyle='--')\n",
    "plt.axvline(x=-4.0, color='lightgrey', linestyle='-')\n",
    "plt.axvline(x=6.0, color='lightgrey', linestyle='-')\n",
    "plt.errorbar(ind, mean, xerr=0.5, yerr=2*std, linestyle='')\n",
    "plt.title('Number of branch closings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Table 6: First-Stage and Reduced-Form estimates \n",
    "# load and prepare data\n",
    "dftest=tab6()[0]\n",
    "exog=tab6()[1]\n",
    "index=tab6()[2]\n",
    "dftest.to_csv('df_table6.csv')\n",
    "\n",
    "# estimte column 1 \n",
    "mod = PanelOLS(dftest.num_closings, dftest[exog], entity_effects=True, time_effects=True, drop_absorbed=True)\n",
    "reg1=mod.fit(cov_type='clustered', clusters=dftest.clustID)\n",
    "\n",
    "# estimte column 2 \n",
    "mod = PanelOLS(dftest.totalbranches, dftest[exog], entity_effects=True, time_effects=True, drop_absorbed=True)\n",
    "reg2=mod.fit(cov_type='clustered', clusters=dftest.clustID)\n",
    "\n",
    "# estimte column 3 \n",
    "mod = PanelOLS(dftest.NumSBL_Rev1, dftest[exog], entity_effects=True, time_effects=True, drop_absorbed=True)\n",
    "reg3=mod.fit(cov_type='clustered', clusters=dftest.clustID)\n",
    "\n",
    "# estimate column 4\n",
    "mod = PanelOLS(dftest.total_origin, dftest[exog], entity_effects=True, time_effects=True, drop_absorbed=True)\n",
    "reg4=mod.fit(cov_type='clustered', clusters=dftest.clustID)\n",
    "\n",
    "# compute baseline mean\n",
    "mean1=np.nanmean(dftest.num_closings)\n",
    "mean2=np.nanmean(dftest.totalbranches)\n",
    "mean3=np.nanmean(dftest.NumSBL_Rev1)\n",
    "mean4=np.nanmean(dftest.total_origin)\n",
    "\n",
    "regressors=index\n",
    "delta=pd.DataFrame(columns=['delta'], index=index)\n",
    "delta['delta']=['<-1','0','1','2','3','4','5','6','>6']\n",
    "print('Table 6: First-Stage and Reduced-Form Estimates \\n')\n",
    "print('   Number of closings:       Total branches:       SBL orginiations:     Mortgage originations:\\n\\n')\n",
    "for i in index:\n",
    "    #print(delta.loc[i, 'delta'])\n",
    "    print('{:<5s}{:>15.4f}{:>20.4f}{:>20.4f}{:>20.4f}'.format(delta.loc[i, 'delta'],reg1.params[i], reg2.params[i], reg3.params[i], reg4.params[i]))\n",
    "    print('{:<5s}{:>15.4f}{:>20.4f}{:>20.4f}{:>20.4f}'.format(' ',reg1.std_errors[i], reg2.std_errors[i], reg3.std_errors[i], reg4.std_errors[i]))\n",
    "print('{:<5s}{:>15.4f}{:>20.4f}{:>20.4f}{:>20.4f}'.format('Mean',mean1, mean2, mean3, mean4))\n",
    "print('{:<5s}{:>15.0f}{:>20.0f}{:>20.0f}{:>20.0f}'.format('Obs',reg1.nobs, reg2.nobs, reg3.nobs, reg4.nobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 6\n",
    "The first two columns of table 6 presents the estimates from the first-stage regressions, while the last two columns show the estimates from the reduced form model. For transparency, the event dummy varies over different time windows indicated by the subscript ($\\delta_x$). The idea of this presentation is to show a general trend. For example, prior and approximate six years after the true merger event the number of closing is negative. In the year of the merger and few years after the dummy estimates are significantly positive indicating that merger induced consolidations take place. Therefore, one cannot argue that the effect of the event dummy is driven by randomness. \n",
    "Due to several implementation issues, the results differ from the results presented by Nguyen (2019). One major issue is that a package for higher dimensional fixed effects regressions is currently not available within the python environment. The corresponding package used in stata is called ‘reghdfe’. The author of the python package ‘econtools’ Daniel M. Sullivan is aware of this problem and plans to implement it in the future (<href>http://www.danielmsullivan.com/pages/tutorial_stata_to_python.html</href>).  I tried to implement fixed effects as a dummy variable regression, which was computational extremely consuming.  As final solution I first reindex the data structure by individual ID and group-year ID. Then as second step I use the ‘linearmodels’ package which allows for two-way fixed effects. \n",
    "Especially, the handling of absorbed observations is different between stata and python. I exported the dataset from stata and load it into python to test whether the data differs (I also checked the data by eyeballing). Both datasets create an issue with multicollinearity in python, thus the error ‘dependent variable matrix does not have full column rank’ appears. The ‘reghdfe’ package in stata is handling missing values and omitted variables differently. Especially, most regression packages using the Cholesky factorization to calculate the regression estimates. In contrast ‘reghdfe’ relies on the method of alternating projections (MAP). The general idea is to refer on the separating hyperplane theorem and using MAP to produce two non-empty, convex and disjoint sets. Therefore, the computation does not produce any errors using stata (in detail see <href>http://scorreia.com/research/hdfe.pdf</href>). I solved this problem by filling missing values with zeros to check whether the collinearity issues can be solved. After this step the estimation is now able to compute estimates.\n",
    "The resulting estimates differ from these presented in Nguyen (2019), but the interpretation and qualitative results still hold. For example, the direction of the point estimates in the first two columns is identically to the authors findings. Indicating that the years after a merger consolidation of branches take place. Due to filling missing values with zeros, the dataset used for estimation slightly differs to the authors one. Therefore, I suspect the estimates of column 3 and 4 differ. To show, that the results published in the paper are the corresponding stata estimates I additionally computed these using the stata ‘reghdfe’ command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Exposure to consolidation and local branch levels\n",
    "mean=fig3()[0]\n",
    "std=fig3()[1]\n",
    "ind=range(-7,9)\n",
    "plt.axhline(y=0.0, color='lightgrey', linestyle='--')\n",
    "plt.axvline(x=0.0, color='lightgrey', linestyle='--')\n",
    "plt.axvline(x=-4.0, color='lightgrey', linestyle='-')\n",
    "plt.axvline(x=6.0, color='lightgrey', linestyle='-')\n",
    "plt.errorbar(ind, mean, xerr=0.5, yerr=2*std, linestyle='')\n",
    "plt.title('Total branches')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'drop_absorbed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a11b3565b0b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Figure 4: Exposure to consolidation and the volume of new lending\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmean1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfig4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mstd1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfig4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmean2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfig4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mstd2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfig4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\UniBonn\\Z_Python_SimulationStudy\\auxiliary.py\u001b[0m in \u001b[0;36mfig4\u001b[1;34m()\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[0mexog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdummylist\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcontrollist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m     \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdummylist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m     \u001b[0mmod1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPanelOLS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdftest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumSBL_Rev1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdftest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexog\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentity_effects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_effects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_absorbed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m     \u001b[0mreg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcov_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'clustered'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdftest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclustID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[0mmean1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'drop_absorbed'"
     ]
    }
   ],
   "source": [
    "# Figure 4: Exposure to consolidation and the volume of new lending\n",
    "mean1=fig4()[0]\n",
    "std1=fig4()[1]\n",
    "mean2=fig4()[2]\n",
    "std2=fig4()[3]\n",
    "ind=range(-7,9)\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.axhline(y=0.0, color='lightgrey', linestyle='--')\n",
    "plt.axvline(x=0.0, color='lightgrey', linestyle='--')\n",
    "plt.axvline(x=-4.0, color='lightgrey', linestyle='-')\n",
    "plt.axvline(x=6.0, color='lightgrey', linestyle='-')\n",
    "plt.errorbar(ind, mean1, xerr=0.5, yerr=2*std1, linestyle='')\n",
    "plt.title('New Small Business loans')\n",
    "plt.subplot(1,2,2)\n",
    "plt.axhline(y=0.0, color='lightgrey', linestyle='--')\n",
    "plt.axvline(x=0.0, color='lightgrey', linestyle='--')\n",
    "plt.axvline(x=-4.0, color='lightgrey', linestyle='-')\n",
    "plt.axvline(x=6.0, color='lightgrey', linestyle='-')\n",
    "plt.errorbar(ind, mean2, xerr=0.5, yerr=2*std2, linestyle='')\n",
    "plt.title('New Mortgages')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 5: The effect of subsequent bank entry on local credit supply\n",
    "plt.figure()\n",
    "\n",
    "#mean=fig3()[0]\n",
    "#mean1=fig4()[0]\n",
    "#mean2=fig4()[2]\n",
    "#ind=range(-7,9)\n",
    "\n",
    "## Small Business Lending\n",
    "plt.subplot(1,2,1)\n",
    "plt.axhline(y=0.0, color='lightgrey', linestyle='--')\n",
    "plt.axvline(x=0.0, color='lightgrey', linestyle='--')\n",
    "plt.axvline(x=-4.0, color='lightgrey', linestyle='-')\n",
    "plt.axvline(x=6.0, color='lightgrey', linestyle='-')\n",
    "plt.scatter(ind, mean1)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(ind, mean)\n",
    "plt.legend((mean1, mean), ('loans', 'totalbranches'),loc='upper center', bbox_to_anchor=(0.5, -0.10))\n",
    "plt.title('New Small Business loans')\n",
    "#plt.show() \n",
    "\n",
    "## Mortgages\n",
    "plt.subplot(1,2,2)\n",
    "plt.axhline(y=0.0, color='lightgrey', linestyle='--')\n",
    "plt.axvline(x=0.0, color='lightgrey', linestyle='--')\n",
    "plt.axvline(x=-4.0, color='lightgrey', linestyle='-')\n",
    "plt.axvline(x=6.0, color='lightgrey', linestyle='-')\n",
    "plt.scatter(ind, mean2)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(ind, mean)\n",
    "plt.legend((mean2, mean), ('loans', 'totalbranches'),loc='upper center', bbox_to_anchor=(0.5, -0.10))\n",
    "plt.title('New Mortgages')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures 2, 3, 4 and 5\n",
    "As already discussed, the results for the first stage estimates slightly correspond to those published in Nguyen (2019). Therefore, the development of the estimates over the time in figures 2 and 3 can be replicated. Note that the scaling of the estimates is different to the authors, since the ‘eclplot’ command in stata differently computes the estimates. I compute the first stage regression estimates and use two times the corresponding standard error to compute the confidence intervals.\n",
    "The reduced form results presented in figure 4, on the other hand, differ from those presented in the paper. This is not surprising, since the estimates in column 3 and 4 of table 6 differ already. Figure 5 uses the data from the figures before. Hence, the development of ‘total branches’ is in line with Nguyen (2019). While the development of the reduced form estimates differs again. \n",
    "Panel A of figure 5 still provides the same intuition as in the paper discussed. After the merger occurs, the number of bank branches declines as well as the commercial lending. Private lending is only affected for a short period. \n",
    "In general, we should pay attention to the results. Since the confidence bands from figures 2, 3 and 4 often cross the zero line, we cannot infer any significant effect from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 7: IV-Estimates of the effect of closings an local credit supply\n",
    "# load and prepare data\n",
    "df=tab7()[0]\n",
    "controllist=tab7()[1]\n",
    "#df.to_csv('df_table7.csv')\n",
    "# compute baseline mean\n",
    "mean1=np.nanmean(df.num_closings)\n",
    "mean2=np.nanmean(df.totalbranches)\n",
    "mean3=np.nanmean(df.NumSBL_Rev1)\n",
    "mean4=np.nanmean(df.total_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS\n",
    "exog='POST_close'\n",
    "#exog=controllist\n",
    "#exog.append('POST_close')\n",
    "#exog = sm.add_constant(df[exog])\n",
    "mod = PanelOLS(df.NumSBL_Rev1, df[exog], entity_effects=True, time_effects=True, drop_absorbed=True)\n",
    "regA1 = mod.fit(cov_type='clustered', clusters=df.clustID)\n",
    "#print('Panel A. OLS: %2.4f (%2.4f)'  %(regA1.params['POST_close'],regA1.std_errors['POST_close']))\n",
    "mod = PanelOLS(df.AmtSBL_Rev1, df[exog], entity_effects=True, time_effects=True, drop_absorbed=True)\n",
    "regA2 = mod.fit(cov_type='clustered', clusters=df.clustID)\n",
    "#print('Panel A. OLS: %2.4f (%2.4f)'  %(regA2.params['POST_close'],regA2.std_errors['POST_close']))\n",
    "mod = PanelOLS(df.total_origin, df[exog], entity_effects=True, time_effects=True, drop_absorbed=True)\n",
    "regA3 = mod.fit(cov_type='clustered', clusters=df.clustID)\n",
    "#print('Panel A. OLS: %2.4f (%2.4f)'  %(regA3.params['POST_close'],regA3.std_errors['POST_close']))\n",
    "mod = PanelOLS(df.loan_amount, df[exog], entity_effects=True, time_effects=True, drop_absorbed=True)\n",
    "regA4 = mod.fit(cov_type='clustered', clusters=df.clustID)\n",
    "#print('Panel A. OLS: %2.4f (%2.4f)'  %(regA4.params['POST_close'],regA4.std_errors['POST_close']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reduced-form \n",
    "exog='POST_expose'\n",
    "#exog=controllist\n",
    "#exog.append('POST_expose')\n",
    "mod = PanelOLS(df.NumSBL_Rev1, df[exog], entity_effects=True, time_effects=True, drop_absorbed=True)\n",
    "regB1 = mod.fit(cov_type='clustered', clusters=df.clustID)\n",
    "#print('Panel B. RF: %2.4f (%2.4f)'  %(regB1.params['POST_expose'],regB1.std_errors['POST_expose']))\n",
    "mod = PanelOLS(df.AmtSBL_Rev1, df[exog], entity_effects=True, time_effects=True, drop_absorbed=True)\n",
    "regB2 = mod.fit(cov_type='clustered', clusters=df.clustID)\n",
    "#print('Panel B. RF: %2.4f (%2.4f)'  %(regB2.params['POST_expose'],regB2.std_errors['POST_expose']))\n",
    "mod = PanelOLS(df.total_origin, df[exog], entity_effects=True, time_effects=True, drop_absorbed=True)\n",
    "regB3 = mod.fit(cov_type='clustered', clusters=df.clustID)\n",
    "#print('Panel B. RF: %2.4f (%2.4f)'  %(regB3.params['POST_expose'],regB3.std_errors['POST_expose']))\n",
    "mod = PanelOLS(df.loan_amount, df[exog], entity_effects=True, time_effects=True, drop_absorbed=True)\n",
    "regB4 = mod.fit(cov_type='clustered', clusters=df.clustID)\n",
    "#print('Panel B. RF: %2.4f (%2.4f)'  %(regB4.params['POST_expose'],regB4.std_errors['POST_expose']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IV\n",
    "#exog=controllist\n",
    "#contrl=\"+\".join(exog)\n",
    "#mod = IV2SLS.from_formula('NumSBL_Rev1 ~ [POST_close ~ POST_expose]+'+contrl, df)\n",
    "mod = IV2SLS.from_formula('NumSBL_Rev1 ~ [POST_close ~ POST_expose]', df)\n",
    "regC1 = mod.fit()\n",
    "#print('Panel B. RF: %2.4f (%2.4f)'  %(regC1.params['POST_close'],regC1.std_errors['POST_close']))\n",
    "mod = IV2SLS.from_formula('AmtSBL_Rev1 ~ [POST_close ~ POST_expose]', df)\n",
    "regC2 = mod.fit()\n",
    "#print('Panel B. RF: %2.4f (%2.4f)'  %(regC2.params['POST_close'],regC2.std_errors['POST_close']))\n",
    "mod = IV2SLS.from_formula('total_origin ~ [POST_close ~ POST_expose]', df)\n",
    "regC3 = mod.fit()\n",
    "#print('Panel B. RF: %2.4f (%2.4f)'  %(regC3.params['POST_close'],regC3.std_errors['POST_close']))\n",
    "mod = IV2SLS.from_formula('loan_amount ~ [POST_close ~ POST_expose]', df)\n",
    "regC4 = mod.fit()\n",
    "#print('Panel B. RF: %2.4f (%2.4f)'  %(regC4.params['POST_close'],regC4.std_errors['POST_close']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table7: IV Estimates of the Effect of Closings on Local Credit Supply\n",
      "\n",
      "Panel A. OLS: -3.4933  -192.1115  -0.4530     95.0195\n",
      "              (0.8237)  (55.2547)  (3.9744)  (636.9604)\n",
      "\n",
      "Panel B.  RF: -4.2428  -301.8764  -0.4534     133.9591\n",
      "              (1.0348)  (85.5112) (3.5049)   (577.4813)\n",
      "\n",
      "Panel C.  IV: 152.5802  7017.1862   340.2441   57576.8465\n",
      "              (3.6203)  (185.0283)  (9.1445)  (1673.3647)\n",
      "\n",
      "Six years cum:                      \n",
      "\n",
      "Mean:          0.1181    4.0116    70.3317    196.4293 \n",
      "\n",
      "Obs:           45864     43723     47210      47198 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Table7: IV Estimates of the Effect of Closings on Local Credit Supply\\n')\n",
    "\n",
    "print('Panel A. OLS: %2.4f  %2.4f  %2.4f     %2.4f'  %(regA1.params['POST_close'],regA2.params['POST_close'],regA3.params['POST_close'],regA4.params['POST_close']))\n",
    "print('              (%2.4f)  (%2.4f)  (%2.4f)  (%2.4f)\\n'  %(regA1.std_errors['POST_close'],regA2.std_errors['POST_close'],regA3.std_errors['POST_close'],regA4.std_errors['POST_close']))\n",
    "\n",
    "print('Panel B.  RF: %2.4f  %2.4f  %2.4f     %2.4f'  %(regB1.params['POST_expose'],regB2.params['POST_expose'],regB3.params['POST_expose'],regB4.params['POST_expose']))\n",
    "print('              (%2.4f)  (%2.4f) (%2.4f)   (%2.4f)\\n'  %(regB1.std_errors['POST_expose'],regB2.std_errors['POST_expose'],regB3.std_errors['POST_expose'],regB4.std_errors['POST_expose']))\n",
    "\n",
    "print('Panel C.  IV: %2.4f  %2.4f   %2.4f   %2.4f'  %(regC1.params['POST_close'],regC2.params['POST_close'],regC3.params['POST_close'],regC4.params['POST_close']))\n",
    "print('              (%2.4f)  (%2.4f)  (%2.4f)  (%2.4f)\\n'  %(regC1.std_errors['POST_close'],regC2.std_errors['POST_close'],regC3.std_errors['POST_close'],regC4.std_errors['POST_close']))\n",
    "\n",
    "print('Six years cum: %2.4s    %2.4s    %2.4s    %2.4s \\n' %('', '', '', ''))\n",
    "print('Mean:          %2.4f    %2.4f    %2.4f    %2.4f \\n' %(mean1, mean2, mean3, mean4))\n",
    "print('Obs:           %2.0f     %2.0f     %2.0f      %2.0f \\n' %(regB1.nobs, regB2.nobs, regB3.nobs, regB4.nobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 7\n",
    "\n",
    "Reproducing table 7 causes the same issue as the estimation used to compute table 6. To test whether the results hold I drop the control variables $X_{it}$. The resulting model looks as follows:\n",
    "\n",
    "$$y_{icmt} = \\alpha_i + (\\gamma_t \\times \\sigma_c) +\\delta_{\\text{POST}} (\\text{POST}_{mt} \\times \\text{Closure}_{icm}) + \\epsilon_{icmt}$$\n",
    "\n",
    "Note that this model is different to those the author used. Thus, the resulting estimates again differ. The direction of the estimates in the first two rows are similar to those Nguyen (2019) presents in table 7, except the last column ‘dollar volume of mortgages'. This is not contrary, since the authors and my estimates are insignificant. The third row presents the estimation of the instrumental variable approach. The ‘linearmodels’ package includes 2SLS regressions but without fixed effects. Again, the estimated model differs to Nguyen (2019):\n",
    "\n",
    "$$ \\text{POST}_{ itcm }=\\beta_0 + \\delta* \\text{POST_Closure}_{ itcm }+\\epsilon_{ itcm }\\\\\n",
    "Y_{itcm}=\\alpha_i + (\\gamma_t \\times \\sigma_c) + \\beta*\\hat{POST}_{ itcm }+\\epsilon_{ictm} $$\n",
    "\n",
    "The estimated effects are significant and positive in contrast to the authors. Since we exclude all control variables and fixed effects the resulting difference is not surprising. Meaning that our estimate may now explain some of the variance our control variables or fixed effects captured before. As in table 6 explained, I additionally computed the results in stata using the ‘reghdfe’ command. \n",
    "Again, I am able to replicate the regression results within stata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion – replication of main results\n",
    "\n",
    "Overall the replication of the results using the python environment is difficult at the current stage of packages available. I am able to run the regressions using the stata commands the author reports. Using python, I often faced issues with missing values, collinearity and implementation of fixed effect models. Surprisingly, the stata ‘redhdfe’ package seems to be quite robust against such errors. Parallel to python I checked the regression models using Julia, R and other stata regression commands. None of these produced the desired regression outputs. To check whether the identification framework correctly identifies the effect of interest, I decided to construct a simulation study in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent contribution - Simulation Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea of this section is to test the identification framework with a stylized simulated dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated Data\n",
    "\n",
    "To later test the identification strategy, we first need to replicate the data structure. This is rather to use the assumptions made for the authors identification than to replicate the underlying sample. Therefore, a major requirement is to implement an endogeneity issue for bank branch closings. This is done by using the same standard normal error term for constructing the bank branch closings ($D$) and the credit supply ($Y$). To apply the difference in differences method and allow for fixed effects, we need to construct a panel data structure. First, I create an empty data frame with 400 individuals (tracts) between the period 1999 and 2013. Then, I assign 60 percent of the individuals to the treatment sample indicated by the exposure variable ($Exp$). The exact merger year is randomly assigned by a discrete uniform distributed integer for each individual. The merger dummy variable ($M$) equals one in the years after a merger is undergone. The tract control variable in the sample used for the analysis does not vary much over time. Thus, I had some multicollinearity issues with tract controls together with the tract fixed effects. To avoid such an issue, I create a time varying control variable with a random mean and standard deviation for each individual (tract). Furthermore, I assigned the 400 individuals to four different groups and create random normal distributed group fixed effects. To account for a general time trend, I define increasing time fixed effects (again normal distributed with $mean=\\frac{year}{2000}$ and $std=0.3$). These effects enter the data as group-by-year fixed effects. In specific, I multiply the group fixed effects with year fixed effects. Finally, the bank branch closings variable ($D$) and the credit supply ($Y$) are computed as described in the formula below. Important to keep in mind for our simulation study are the true parameters of interest $\\beta=0.99$ and $\\delta=0.5$. These parameters are used for the data generating process and we later are interested in best estimation of these true parameters.\n",
    "\n",
    "$$\n",
    "D_{itcm} = 0.5 * M_{mt} + 0.2*X_{it} +\\epsilon_{itcm} \\\\\n",
    "Y_{itcm} = \\alpha_i + (\\gamma_t \\times \\sigma_c) + 0.99 * D_{it} + 0.3*X_{it} + \\epsilon_{itcm}\n",
    "$$\n",
    "\n",
    "After reading the generated dataset, I define the difference in differences treatment dummy variable by multiplying the merger dummy ($M$) with the exposure dummy ($Exp$). Next we need to set the index to later apply the fixed effects using the linearmodels package. The data structure is presented in the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>D</th>\n",
       "      <th>M</th>\n",
       "      <th>Exp</th>\n",
       "      <th>X</th>\n",
       "      <th>t</th>\n",
       "      <th>iID</th>\n",
       "      <th>groupID</th>\n",
       "      <th>DD</th>\n",
       "      <th>gtID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indivID</th>\n",
       "      <th>group_timeID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>-1.75614</td>\n",
       "      <td>0.191607</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.67211</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.30991</td>\n",
       "      <td>2.18673</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.44031</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.81534</td>\n",
       "      <td>0.477901</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.68206</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.34243</td>\n",
       "      <td>0.913258</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.83203</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.263266</td>\n",
       "      <td>1.48044</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.34872</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Y         D  M  Exp        X       t  iID  \\\n",
       "indivID group_timeID                                                     \n",
       "0.0     0             -1.75614  0.191607  0    1  3.67211  1999.0  0.0   \n",
       "        1              2.30991   2.18673  0    1  8.44031  2000.0  0.0   \n",
       "        2             -2.81534  0.477901  0    1  1.68206  2001.0  0.0   \n",
       "        3             -1.34243  0.913258  1    1  5.83203  2002.0  0.0   \n",
       "        4             0.263266   1.48044  1    1  6.34872  2003.0  0.0   \n",
       "\n",
       "                      groupID  DD  gtID  \n",
       "indivID group_timeID                     \n",
       "0.0     0                   1   0     0  \n",
       "        1                   1   0     1  \n",
       "        2                   1   0     2  \n",
       "        3                   1   1     3  \n",
       "        4                   1   1     4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "df=panel_sample()\n",
    "#df.to_csv('df_sample.csv')\n",
    "\n",
    "df['DD']=df.M*df.Exp\n",
    "df['indivID']=df['iID'].copy()\n",
    "df['gtID']=df['group_timeID'].copy()\n",
    "df.set_index(['indivID', 'group_timeID'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The naive OLS approach\n",
    "\n",
    "As already motivated before, we have an endogeneity issue with bank branch closings ($D$) and credit supply ($Y$). Therefore, applying a naïve OLS regression should yield biased results of our estimates.\n",
    "\n",
    "$$\n",
    "Y_{itc} = \\alpha_i + (\\gamma_t \\times \\sigma_c) + \\beta* D_{it} + \\epsilon_{itc}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The naive approach, which should be biased \n",
      " 2.1313 \n",
      " (0.0050)\n"
     ]
    }
   ],
   "source": [
    "mod = PanelOLS(df.Y, df['D'], entity_effects=True, time_effects=True)\n",
    "res = mod.fit(cov_type='clustered', clusters=df.groupID)\n",
    "print('The naive approach, which should be biased \\n %2.4f \\n (%2.4f)' %(res.params,res.std_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The naive IV approach\n",
    "\n",
    "To solve the endogeneity issue, we apply an instrumental variable approach by instrumenting bank branch closings ($D$) with our exogenous instrument post-merger consolidation ($M$). The predicted values from the first stage regression do not include the biased error term which causes the endogeneity. In the second stage of this analysis we regress credit supply ($Y$) on the predicted values to identify the effect of interest. Since we do not control for time-varying individual tract characteristics, the estimation should be biased. As we can see, the approach correctly identifies the first stage relationship, but is downward biased due to the lack of control for time-varying individual characteristics.\n",
    "\n",
    "$$\n",
    "D_{itm}=\\alpha_i + \\gamma_t + \\delta* M_{mt}+\\epsilon_{it}\\\\\n",
    "Y_{itcm}=\\alpha_i + (\\gamma_t \\times \\sigma_c) + \\beta*\\hat{D}_{itm}+\\epsilon_{ictm}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The naive IV approach without tract controls \n",
      " First stage: \n",
      " 0.4983 \n",
      " (0.0244) \n",
      " Second stage: \n",
      " 0.8246 \n",
      " (0.1433)\n"
     ]
    }
   ],
   "source": [
    "df['indivID']=df['iID'].copy()\n",
    "df['year']=df['t'].copy()\n",
    "df.set_index(['indivID', 'year'], inplace=True)\n",
    "mod1 = PanelOLS(df.D, df.M,entity_effects=True, time_effects=True)\n",
    "res1 = mod1.fit(cov_type='clustered', clusters=df.groupID)\n",
    "df['predicted']=res1.predict()\n",
    "df['indivID']=df['iID'].copy()\n",
    "df['group_timeID']=df['gtID'].copy()\n",
    "df.set_index(['indivID', 'group_timeID'], inplace=True)\n",
    "mod2 = PanelOLS(df.Y, df.predicted, entity_effects=True, time_effects=True)\n",
    "res2 = mod2.fit(cov_type='clustered', clusters=df.groupID)\n",
    "print('The naive IV approach without tract controls \\n First stage: \\n %2.4f \\n (%2.4f) \\n Second stage: \\n %2.4f \\n (%2.4f)' %(res1.params['M'],res1.std_errors['M'],res2.params['predicted'],res2.std_errors['predicted']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The reduced form difference in differences approach\n",
    "\n",
    "This approach measures the effect of merger induced bank branch closings. Hence, it is likely to underestimate the true effect of bank branch closings. Again, we are not controlling for time-varying individual tract controls. As the result suggest, there are two drivers of underestimation. First, we condition on merger induced closings which represents only a subset of all closings. Second, we miss to control for time-varying individual characteristics ($X_{it}$). \n",
    "\n",
    "$$\n",
    "Y_{itcm}=\\alpha_i + (\\gamma_t \\times \\sigma_c) + \\delta*( M_{mt}\\times Exp_{icm})+\\epsilon_{ictm}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reduced form (DD) with Exposure to merger as instrument \n",
      " 0.4109 \n",
      " (0.0714)\n"
     ]
    }
   ],
   "source": [
    "mod = PanelOLS(df.Y, df['DD'], entity_effects=True, time_effects=True)\n",
    "res = mod.fit(cov_type='clustered', clusters=df.groupID)\n",
    "print('The reduced form (DD) with Exposure to merger as instrument \\n %2.4f \\n (%2.4f)' %(res.params,res.std_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The reduced form difference in differences approach with controls\n",
    "This approach is used by Nguyen (2019) to estimate the effect of merger induced bank branch closings on local credit supply. The author is aware of the issue, that she is likely to underestimate the true effect by conditioning of a subset.  The result approximately correctly identifies the true effect of merger induced closings.\n",
    "\n",
    "\n",
    "$$\n",
    "Y_{itcm}=\\alpha_i + (\\gamma_t \\times \\sigma_c) + \\delta*( M_{mt}\\times Exp_{icm})+\\phi X_{it}+\\epsilon_{ictm}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reduced form (DD) with Exposure to merger as instrument and tract controls \n",
      " 0.4897 \n",
      " (0.0394)\n",
      "\n",
      "Note: The DD framework identifies only merger induced effects of branch closings on credit supply. Thus, the true LATE (0.99)\n",
      "is underestimated as in the indentification section discussed.\n",
      "The true effect is 0.5 thus the authors framework should yield reliable results  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod = PanelOLS(df.Y, df[['DD','X']], entity_effects=True, time_effects=True)\n",
    "res = mod.fit(cov_type='clustered', clusters=df.groupID)\n",
    "print('The reduced form (DD) with Exposure to merger as instrument and tract controls \\n %2.4f \\n (%2.4f)\\n' %(res.params[0],res.std_errors[0]))\n",
    "print('Note: The DD framework identifies only merger induced effects of branch closings on credit supply. Thus, the true LATE (0.99)\\nis underestimated as in the indentification section discussed.\\nThe true effect is 0.5 thus the authors framework should yield reliable results  \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV approach with controls\n",
    "\n",
    "Nguyen (2019) finally estimates the effect of interest using an instrumental variable framework along with tract controls, individual and group-by-year fixed effects. This approach is able to approximately estimate the true effect of bank branch closings ($0.99$).\n",
    "\n",
    "$$\n",
    "D_{it}=\\alpha_i + \\gamma_t + \\phi X_{it} + \\delta* {DD}_{it}\\\\\n",
    "Y_{itcm}=\\alpha_i + (\\gamma_t \\times \\sigma_c) + \\beta*\\hat{D}_{it} + \\phi X_{it} +\\epsilon_{ictm}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors IV approach, which includes tract controls \n",
      " First stage: \n",
      " 0.5066 \n",
      " (0.0178) \n",
      " Second stage: \n",
      " 0.9667 \n",
      " (0.0777)\n"
     ]
    }
   ],
   "source": [
    "df['indivID']=df['iID'].copy()\n",
    "df['year']=df['t'].copy()\n",
    "df.set_index(['indivID', 'year'], inplace=True)\n",
    "mod1 = PanelOLS(df.D, df[['DD','X']],entity_effects=True, time_effects=True)\n",
    "res1 = mod1.fit(cov_type='clustered', clusters=df.groupID)\n",
    "df['predicted']=res1.predict()\n",
    "df['indivID']=df['iID'].copy()\n",
    "df['group_timeID']=df['gtID'].copy()\n",
    "df.set_index(['indivID', 'group_timeID'], inplace=True)\n",
    "mod2 = PanelOLS(df.Y, df[['predicted','X']], entity_effects=True, time_effects=True)\n",
    "res2 = mod2.fit(cov_type='clustered', clusters=df.groupID)\n",
    "print('The authors IV approach, which includes tract controls \\n First stage: \\n %2.4f \\n (%2.4f) \\n Second stage: \\n %2.4f \\n (%2.4f)' %(res1.params['DD'],res1.std_errors['DD'],res2.params['predicted'],res2.std_errors['predicted']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion of the Simulation Study\n",
    "\n",
    "Simulation studies help us to deepen the understanding of the underlying data structure and assumptions. With the data generating process in mind one is able to test different estimation setups to later identify the best approach. This simulation study shows the strengths and weaknesses of the different approaches applied in the paper by Nguyen (2019). As she discussed, the reduced form systematically underestimates the true effect of interest. Applying an instrumental variable approach yields reliable results. Thus, the identification strategy seems to be appropriate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Akerlof, G. A. (1970). The market for lemons: Quality and the market mechanism. Quarterly. Journal Economics, 84, 488-500.*\n",
    "\n",
    "*Angrist, J. D., & Pischke, J. S. (2008). Mostly harmless econometrics: An empiricist's companion. Princeton university press.*\n",
    "\n",
    "*Frölich, M., & Sperlich, S. (2019). Impact evaluation. Cambridge University Press.*\n",
    "\n",
    "*Nguyen, H. L. Q. (2019). Are credit markets still local? evidence from bank branch closings. American Economic Journal: Applied Economics, 11(1), 1-32.*\n",
    "\n",
    "*Stiglitz, J. E., & Weiss, A. (1981). Credit rationing in markets with imperfect information. The American economic review, 71(3), 393-410.*\n",
    "\n",
    "*Wooldridge, J. M. (2015). Introductory econometrics: A modern approach. Nelson Education.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
